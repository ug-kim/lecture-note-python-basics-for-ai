{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment4_answer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UL2tS-EQZKN"
      },
      "source": [
        "# Assignment 4 : Body Landmark Localization using Hourglass Network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JNiQ2H2nS-r"
      },
      "source": [
        "# Seed\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "torch.manual_seed(0)\r\n",
        "torch.cuda.manual_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "# Ignore warnings\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nCnWRyQh2P"
      },
      "source": [
        "### **4.1 Hourglass Module Implementation**\r\n",
        "\r\n",
        "아래 **Fig. 3.**에 표현되어 있는 **Hourglass module**을 구현하고자 합니다. 이미 선언되어 있는 layer들을 이용하여 figure 상의 layer 구성과 동일하게 tensor가 forward 될 수 있도록 ```def forward``` 부분을 완성해주세요.\r\n",
        "\r\n",
        "\r\n",
        "- Fig. 4. Right에 표현된 supervision layer는 해당 과제에서는 고려하지 않습니다.\r\n",
        "- The figures are from [the original hourglass paper](https://arxiv.org/abs/1603.06937) [Newell et al.].\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?id=19-S7TwZ62joUR8W9031xjn3jMZyTevpw'  width=\"700\">\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?id=1ols0VZ7TGZCMDM7sKzCJq3bByHsOU9up'  width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRXBDC0qkhkP"
      },
      "source": [
        "아래의 코드는 Hourglass 모듈을 나타내는 클래스입니다. 위의 Figure를 참고하여 **TO DO** 과제를 채워주세요 :)\r\n",
        "\r\n",
        "- **TO DO** : ```class Hourglass```는 하나의 Hourglass 모듈을 의미하며 이전에 선언한 ```class ResidualBlock```을 기본 convolution block으로 사용합니다. Hourglass 내부에 사용되는 layer는 이미 ```def __init__```에 선언이 되어 있지만 `**``def forward``` 부분은 완성되지 않아 선언된 layer들을 구성에 맞게 연결**해주어야 합니다. Fig. 3.을 참고하여 Hourglass 모듈을 올바르게 구현해주세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zehDRYDCkTsK"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class ResidualBlock(nn.Module):\r\n",
        "  def __init__(self, num_channels=256):\r\n",
        "    super(ResidualBlock, self).__init__()\r\n",
        "\r\n",
        "    self.bn1 = nn.BatchNorm2d(num_channels)\r\n",
        "    self.conv1 = nn.Conv2d(num_channels, num_channels//2, kernel_size=1, bias=True)\r\n",
        "\r\n",
        "    self.bn2 = nn.BatchNorm2d(num_channels//2)\r\n",
        "    self.conv2 = nn.Conv2d(num_channels//2, num_channels//2, kernel_size=3, stride=1,\r\n",
        "                              padding=1, bias=True)\r\n",
        "\r\n",
        "    self.bn3 = nn.BatchNorm2d(num_channels//2)\r\n",
        "    self.conv3 = nn.Conv2d(num_channels//2, num_channels, kernel_size=1, bias=True)\r\n",
        "\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    residual = x\r\n",
        "\r\n",
        "    out = self.bn1(x)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.conv1(out)\r\n",
        "\r\n",
        "    out = self.bn2(out)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.conv2(out)\r\n",
        "\r\n",
        "    out = self.bn3(out)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.conv3(out)\r\n",
        "\r\n",
        "    out += residual\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiK_5kEYQsda"
      },
      "source": [
        "class Hourglass(nn.Module):\r\n",
        "  def __init__(self, block, num_channels=256):\r\n",
        "    super(Hourglass, self).__init__()\r\n",
        "\r\n",
        "    self.downconv_1 = block(num_channels)\r\n",
        "    self.pool_1 = nn.MaxPool2d(kernel_size=2)\r\n",
        "    self.downconv_2 = block(num_channels)\r\n",
        "    self.pool_2 = nn.MaxPool2d(kernel_size=2)\r\n",
        "    self.downconv_3 = block(num_channels)\r\n",
        "    self.pool_3 = nn.MaxPool2d(kernel_size=2)\r\n",
        "    self.downconv_4 = block(num_channels)\r\n",
        "    self.pool_4 = nn.MaxPool2d(kernel_size=2)\r\n",
        "\r\n",
        "    self.midconv_1 = block(num_channels)\r\n",
        "    self.midconv_2 = block(num_channels)\r\n",
        "    self.midconv_3 = block(num_channels)\r\n",
        "    \r\n",
        "    self.skipconv_1 = block(num_channels)\r\n",
        "    self.skipconv_2 = block(num_channels)\r\n",
        "    self.skipconv_3 = block(num_channels)\r\n",
        "    self.skipconv_4 = block(num_channels)\r\n",
        "\r\n",
        "    self.upconv_1 = block(num_channels)\r\n",
        "    self.upconv_2 = block(num_channels)\r\n",
        "    self.upconv_3 = block(num_channels)\r\n",
        "    self.upconv_4 = block(num_channels)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x1 = self.downconv_1(x)\r\n",
        "    x  = self.pool_1(x1)\r\n",
        "\r\n",
        "    '''======================================================='''\r\n",
        "    '''======================== TO DO ========================'''\r\n",
        "    x2 = self.downconv_2(x)\r\n",
        "    x  = self.pool_2(x2)\r\n",
        "    x3 = self.downconv_3(x)\r\n",
        "    x  = self.pool_3(x3)\r\n",
        "    x4 = self.downconv_4(x)\r\n",
        "    x  = self.pool_4(x4)\r\n",
        "\r\n",
        "    x = self.midconv_1(x)\r\n",
        "    x = self.midconv_2(x)\r\n",
        "    x = self.midconv_3(x)\r\n",
        "\r\n",
        "    x4 = self.skipconv_1(x4)\r\n",
        "    x = F.upsample(x, scale_factor=2)\r\n",
        "    x = x + x4\r\n",
        "    x = self.upconv_1(x)\r\n",
        "\r\n",
        "    x3 = self.skipconv_1(x3)\r\n",
        "    x = F.upsample(x, scale_factor=2)\r\n",
        "    x = x + x3\r\n",
        "    x = self.upconv_2(x)\r\n",
        "\r\n",
        "    x2 = self.skipconv_1(x2)\r\n",
        "    x = F.upsample(x, scale_factor=2)\r\n",
        "    x = x + x2\r\n",
        "    x = self.upconv_3(x)\r\n",
        "\r\n",
        "    x1 = self.skipconv_1(x1)\r\n",
        "    x = F.upsample(x, scale_factor=2)\r\n",
        "    x = x + x1\r\n",
        "    x = self.upconv_4(x)\r\n",
        "    '''======================== TO DO ========================'''\r\n",
        "    '''======================================================='''\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZfXyBDfldbE"
      },
      "source": [
        "----\r\n",
        "[torchsummary](https://github.com/sksq96/pytorch-summary)는 PyTorch로 구현한 네트워크를 직관적으로 확인할 수 있는 라이브러리입니다.\r\n",
        "\r\n",
        "해당 라이브러리를 이용하여 각 feature map의 dimension과 각각의 layer가 몇개의 parameter 수를 가지고 있는지 확인해 봅시다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1-T5hNpYhI1",
        "outputId": "9acfa65b-2cbe-4ea7-bb23-a41d975e08fd"
      },
      "source": [
        "# Let's summary the implemented hourglass architecture using torchsummary library.\n",
        "hg = Hourglass(ResidualBlock)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(hg, input_size=(256,64,64), device='cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1          [-1, 256, 64, 64]             512\n",
            "              ReLU-2          [-1, 256, 64, 64]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]          32,896\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "              ReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 128, 64, 64]         147,584\n",
            "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
            "              ReLU-8          [-1, 128, 64, 64]               0\n",
            "            Conv2d-9          [-1, 256, 64, 64]          33,024\n",
            "    ResidualBlock-10          [-1, 256, 64, 64]               0\n",
            "        MaxPool2d-11          [-1, 256, 32, 32]               0\n",
            "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
            "             ReLU-13          [-1, 256, 32, 32]               0\n",
            "           Conv2d-14          [-1, 128, 32, 32]          32,896\n",
            "      BatchNorm2d-15          [-1, 128, 32, 32]             256\n",
            "             ReLU-16          [-1, 128, 32, 32]               0\n",
            "           Conv2d-17          [-1, 128, 32, 32]         147,584\n",
            "      BatchNorm2d-18          [-1, 128, 32, 32]             256\n",
            "             ReLU-19          [-1, 128, 32, 32]               0\n",
            "           Conv2d-20          [-1, 256, 32, 32]          33,024\n",
            "    ResidualBlock-21          [-1, 256, 32, 32]               0\n",
            "        MaxPool2d-22          [-1, 256, 16, 16]               0\n",
            "      BatchNorm2d-23          [-1, 256, 16, 16]             512\n",
            "             ReLU-24          [-1, 256, 16, 16]               0\n",
            "           Conv2d-25          [-1, 128, 16, 16]          32,896\n",
            "      BatchNorm2d-26          [-1, 128, 16, 16]             256\n",
            "             ReLU-27          [-1, 128, 16, 16]               0\n",
            "           Conv2d-28          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
            "             ReLU-30          [-1, 128, 16, 16]               0\n",
            "           Conv2d-31          [-1, 256, 16, 16]          33,024\n",
            "    ResidualBlock-32          [-1, 256, 16, 16]               0\n",
            "        MaxPool2d-33            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "             ReLU-35            [-1, 256, 8, 8]               0\n",
            "           Conv2d-36            [-1, 128, 8, 8]          32,896\n",
            "      BatchNorm2d-37            [-1, 128, 8, 8]             256\n",
            "             ReLU-38            [-1, 128, 8, 8]               0\n",
            "           Conv2d-39            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-40            [-1, 128, 8, 8]             256\n",
            "             ReLU-41            [-1, 128, 8, 8]               0\n",
            "           Conv2d-42            [-1, 256, 8, 8]          33,024\n",
            "    ResidualBlock-43            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-44            [-1, 256, 4, 4]               0\n",
            "      BatchNorm2d-45            [-1, 256, 4, 4]             512\n",
            "             ReLU-46            [-1, 256, 4, 4]               0\n",
            "           Conv2d-47            [-1, 128, 4, 4]          32,896\n",
            "      BatchNorm2d-48            [-1, 128, 4, 4]             256\n",
            "             ReLU-49            [-1, 128, 4, 4]               0\n",
            "           Conv2d-50            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 4, 4]             256\n",
            "             ReLU-52            [-1, 128, 4, 4]               0\n",
            "           Conv2d-53            [-1, 256, 4, 4]          33,024\n",
            "    ResidualBlock-54            [-1, 256, 4, 4]               0\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 128, 4, 4]          32,896\n",
            "      BatchNorm2d-58            [-1, 128, 4, 4]             256\n",
            "             ReLU-59            [-1, 128, 4, 4]               0\n",
            "           Conv2d-60            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-61            [-1, 128, 4, 4]             256\n",
            "             ReLU-62            [-1, 128, 4, 4]               0\n",
            "           Conv2d-63            [-1, 256, 4, 4]          33,024\n",
            "    ResidualBlock-64            [-1, 256, 4, 4]               0\n",
            "      BatchNorm2d-65            [-1, 256, 4, 4]             512\n",
            "             ReLU-66            [-1, 256, 4, 4]               0\n",
            "           Conv2d-67            [-1, 128, 4, 4]          32,896\n",
            "      BatchNorm2d-68            [-1, 128, 4, 4]             256\n",
            "             ReLU-69            [-1, 128, 4, 4]               0\n",
            "           Conv2d-70            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-71            [-1, 128, 4, 4]             256\n",
            "             ReLU-72            [-1, 128, 4, 4]               0\n",
            "           Conv2d-73            [-1, 256, 4, 4]          33,024\n",
            "    ResidualBlock-74            [-1, 256, 4, 4]               0\n",
            "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
            "             ReLU-76            [-1, 256, 8, 8]               0\n",
            "           Conv2d-77            [-1, 128, 8, 8]          32,896\n",
            "      BatchNorm2d-78            [-1, 128, 8, 8]             256\n",
            "             ReLU-79            [-1, 128, 8, 8]               0\n",
            "           Conv2d-80            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-81            [-1, 128, 8, 8]             256\n",
            "             ReLU-82            [-1, 128, 8, 8]               0\n",
            "           Conv2d-83            [-1, 256, 8, 8]          33,024\n",
            "    ResidualBlock-84            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-85            [-1, 256, 8, 8]             512\n",
            "             ReLU-86            [-1, 256, 8, 8]               0\n",
            "           Conv2d-87            [-1, 128, 8, 8]          32,896\n",
            "      BatchNorm2d-88            [-1, 128, 8, 8]             256\n",
            "             ReLU-89            [-1, 128, 8, 8]               0\n",
            "           Conv2d-90            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 8, 8]             256\n",
            "             ReLU-92            [-1, 128, 8, 8]               0\n",
            "           Conv2d-93            [-1, 256, 8, 8]          33,024\n",
            "    ResidualBlock-94            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
            "             ReLU-96          [-1, 256, 16, 16]               0\n",
            "           Conv2d-97          [-1, 128, 16, 16]          32,896\n",
            "      BatchNorm2d-98          [-1, 128, 16, 16]             256\n",
            "             ReLU-99          [-1, 128, 16, 16]               0\n",
            "          Conv2d-100          [-1, 128, 16, 16]         147,584\n",
            "     BatchNorm2d-101          [-1, 128, 16, 16]             256\n",
            "            ReLU-102          [-1, 128, 16, 16]               0\n",
            "          Conv2d-103          [-1, 256, 16, 16]          33,024\n",
            "   ResidualBlock-104          [-1, 256, 16, 16]               0\n",
            "     BatchNorm2d-105          [-1, 256, 16, 16]             512\n",
            "            ReLU-106          [-1, 256, 16, 16]               0\n",
            "          Conv2d-107          [-1, 128, 16, 16]          32,896\n",
            "     BatchNorm2d-108          [-1, 128, 16, 16]             256\n",
            "            ReLU-109          [-1, 128, 16, 16]               0\n",
            "          Conv2d-110          [-1, 128, 16, 16]         147,584\n",
            "     BatchNorm2d-111          [-1, 128, 16, 16]             256\n",
            "            ReLU-112          [-1, 128, 16, 16]               0\n",
            "          Conv2d-113          [-1, 256, 16, 16]          33,024\n",
            "   ResidualBlock-114          [-1, 256, 16, 16]               0\n",
            "     BatchNorm2d-115          [-1, 256, 32, 32]             512\n",
            "            ReLU-116          [-1, 256, 32, 32]               0\n",
            "          Conv2d-117          [-1, 128, 32, 32]          32,896\n",
            "     BatchNorm2d-118          [-1, 128, 32, 32]             256\n",
            "            ReLU-119          [-1, 128, 32, 32]               0\n",
            "          Conv2d-120          [-1, 128, 32, 32]         147,584\n",
            "     BatchNorm2d-121          [-1, 128, 32, 32]             256\n",
            "            ReLU-122          [-1, 128, 32, 32]               0\n",
            "          Conv2d-123          [-1, 256, 32, 32]          33,024\n",
            "   ResidualBlock-124          [-1, 256, 32, 32]               0\n",
            "     BatchNorm2d-125          [-1, 256, 32, 32]             512\n",
            "            ReLU-126          [-1, 256, 32, 32]               0\n",
            "          Conv2d-127          [-1, 128, 32, 32]          32,896\n",
            "     BatchNorm2d-128          [-1, 128, 32, 32]             256\n",
            "            ReLU-129          [-1, 128, 32, 32]               0\n",
            "          Conv2d-130          [-1, 128, 32, 32]         147,584\n",
            "     BatchNorm2d-131          [-1, 128, 32, 32]             256\n",
            "            ReLU-132          [-1, 128, 32, 32]               0\n",
            "          Conv2d-133          [-1, 256, 32, 32]          33,024\n",
            "   ResidualBlock-134          [-1, 256, 32, 32]               0\n",
            "     BatchNorm2d-135          [-1, 256, 64, 64]             512\n",
            "            ReLU-136          [-1, 256, 64, 64]               0\n",
            "          Conv2d-137          [-1, 128, 64, 64]          32,896\n",
            "     BatchNorm2d-138          [-1, 128, 64, 64]             256\n",
            "            ReLU-139          [-1, 128, 64, 64]               0\n",
            "          Conv2d-140          [-1, 128, 64, 64]         147,584\n",
            "     BatchNorm2d-141          [-1, 128, 64, 64]             256\n",
            "            ReLU-142          [-1, 128, 64, 64]               0\n",
            "          Conv2d-143          [-1, 256, 64, 64]          33,024\n",
            "   ResidualBlock-144          [-1, 256, 64, 64]               0\n",
            "     BatchNorm2d-145          [-1, 256, 64, 64]             512\n",
            "            ReLU-146          [-1, 256, 64, 64]               0\n",
            "          Conv2d-147          [-1, 128, 64, 64]          32,896\n",
            "     BatchNorm2d-148          [-1, 128, 64, 64]             256\n",
            "            ReLU-149          [-1, 128, 64, 64]               0\n",
            "          Conv2d-150          [-1, 128, 64, 64]         147,584\n",
            "     BatchNorm2d-151          [-1, 128, 64, 64]             256\n",
            "            ReLU-152          [-1, 128, 64, 64]               0\n",
            "          Conv2d-153          [-1, 256, 64, 64]          33,024\n",
            "   ResidualBlock-154          [-1, 256, 64, 64]               0\n",
            "================================================================\n",
            "Total params: 3,217,920\n",
            "Trainable params: 3,217,920\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 226.44\n",
            "Params size (MB): 12.28\n",
            "Estimated Total Size (MB): 242.71\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWXn9Pncl6Qt"
      },
      "source": [
        "----\r\n",
        "아래는 네트워크 구현이 정확하게 되었는지를 확인하기 위한 코드입니다.\r\n",
        "\r\n",
        "<br>Assignment1와 마찬가지로 아래의 코드는 **Hourglass 모듈의 중간 feature map들의 shape**을 바탕으로 일련의 연산을 수행하여 **하나의 값**을 계산합니다.\r\n",
        "<br>채점을 위하여 아래의 코드 결과로 얻은 값을 **edwith에 제출**해주세요 :)\r\n",
        "\r\n",
        "(주의 : 정확한 채점을 위하여 아래 코드는 수정하지 마세요!)\r\n",
        "\r\n",
        "<br>예를 들어, 아래와 같은 실행 결과를 얻으셨다면 edwith 퀴즈에 7777을 선택해주세요.\r\n",
        "```python\r\n",
        "\"Your answer is : 7777\"\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YURxVV1jmDkM"
      },
      "source": [
        "import base64, copy\r\n",
        "\r\n",
        "class Calculator:\r\n",
        "  '''\r\n",
        "  NOTE : DO NOT MODIFY THE CODE BELOW.\r\n",
        "  '''\r\n",
        "  def __init__(self, model):\r\n",
        "    self.answer = 0\r\n",
        "    modules = [b'c2tpcGNvbnZfMg==\\n', b'dXBjb252XzM=\\n']\r\n",
        "    layer = b'Y29udjI=\\n'\r\n",
        "    for m in modules:\r\n",
        "      self.hook = model._modules[base64.decodebytes(m).decode()]._modules[base64.decodebytes(layer).decode()].register_forward_hook(self.hook_fn)\r\n",
        "    \r\n",
        "  def hook_fn(self, module, input, output):\r\n",
        "    self.answer += self._get_answer(output)\r\n",
        "  \r\n",
        "  def _get_answer(self, l):\r\n",
        "    _, A, B, C = l.shape\r\n",
        "    return A*(B-C//3)\r\n",
        "    \r\n",
        "  def unregister_forward_hook(self):\r\n",
        "    self.hook.remove()\r\n",
        "  \r\n",
        "\r\n",
        "def calc_anwser(model):\r\n",
        "  # NOTE : DO NOT MODIFY THE CODE BELOW.\r\n",
        "  model_test = copy.deepcopy(model)\r\n",
        "  ans_calculator = Calculator(model_test)\r\n",
        "\r\n",
        "  x = torch.rand(1,256,64,64)\r\n",
        "  model_test(x)\r\n",
        "\r\n",
        "  print(\"Your answer is : %d\" % ans_calculator.answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Q6L3X6mEiP",
        "outputId": "4bea3204-74ac-4c83-ddc0-2c7878a78f78"
      },
      "source": [
        "calc_anwser(hg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your answer is : 2816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOETu9-yd_CV"
      },
      "source": [
        "----\n",
        "### **4.2 Human Pose Estimation**\n",
        "\n",
        "[Stacked Hourglass Network](https://arxiv.org/abs/1603.06937)를 이용하여 human pose estimation task를 수행하여 봅시다!\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1gJPaBX8uVWY9FnNRf2H3rmP1xYsd73eR'  width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp1OcSEZwhcn"
      },
      "source": [
        "##### **>>> 4.2.1 Stacked Hourglass Network**\r\n",
        "아래 코드는 stacked hourglass network의 전체 코드입니다. ([원본 github 링크](https://github.com/bearpaw/pytorch-pose))\r\n",
        "\r\n",
        "- 3.1에서 Hourglass 모듈을 구현할 때 일일이 layer를 쌓는 것 대신에 for loop와 [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)를 이용하여 더욱 직관적이고 명료한 코드 작성이 가능하다는 것도 한번 확인해보세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooCFffn9xavs"
      },
      "source": [
        "'''\r\n",
        "Hourglass network inserted in the pre-activated Resnet\r\n",
        "Use lr=0.01 for current version\r\n",
        "(c) YANG, Wei\r\n",
        "'''\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# from .preresnet import BasicBlock, Bottleneck\r\n",
        "\r\n",
        "\r\n",
        "__all__ = ['HourglassNet', 'hg']\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    expansion = 2\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "\r\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\r\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\r\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\r\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n",
        "                               padding=1, bias=True)\r\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\r\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "\r\n",
        "        out = self.bn1(x)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv1(out)\r\n",
        "\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv2(out)\r\n",
        "\r\n",
        "        out = self.bn3(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.conv3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            residual = self.downsample(x)\r\n",
        "\r\n",
        "        out += residual\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Hourglass(nn.Module):\r\n",
        "    def __init__(self, block, num_blocks, planes, depth):\r\n",
        "        super(Hourglass, self).__init__()\r\n",
        "        self.depth = depth\r\n",
        "        self.block = block\r\n",
        "        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\r\n",
        "\r\n",
        "    def _make_residual(self, block, num_blocks, planes):\r\n",
        "        layers = []\r\n",
        "        for i in range(0, num_blocks):\r\n",
        "            layers.append(block(planes*block.expansion, planes))\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def _make_hour_glass(self, block, num_blocks, planes, depth):\r\n",
        "        hg = []\r\n",
        "        for i in range(depth):\r\n",
        "            res = []\r\n",
        "            for j in range(3):\r\n",
        "                res.append(self._make_residual(block, num_blocks, planes))\r\n",
        "            if i == 0:\r\n",
        "                res.append(self._make_residual(block, num_blocks, planes))\r\n",
        "            hg.append(nn.ModuleList(res))\r\n",
        "        return nn.ModuleList(hg)\r\n",
        "\r\n",
        "    def _hour_glass_forward(self, n, x):\r\n",
        "        up1 = self.hg[n-1][0](x)\r\n",
        "        low1 = F.max_pool2d(x, 2, stride=2)\r\n",
        "        low1 = self.hg[n-1][1](low1)\r\n",
        "\r\n",
        "        if n > 1:\r\n",
        "            low2 = self._hour_glass_forward(n-1, low1)\r\n",
        "        else:\r\n",
        "            low2 = self.hg[n-1][3](low1)\r\n",
        "        low3 = self.hg[n-1][2](low2)\r\n",
        "        up2 = F.interpolate(low3, scale_factor=2)\r\n",
        "        out = up1 + up2\r\n",
        "        return out\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self._hour_glass_forward(self.depth, x)\r\n",
        "\r\n",
        "\r\n",
        "class HourglassNet(nn.Module):\r\n",
        "    '''Hourglass model from Newell et al ECCV 2016'''\r\n",
        "    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16):\r\n",
        "        super(HourglassNet, self).__init__()\r\n",
        "\r\n",
        "        self.inplanes = 64\r\n",
        "        self.num_feats = 128\r\n",
        "        self.num_stacks = num_stacks\r\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\r\n",
        "                               bias=True)\r\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.layer1 = self._make_residual(block, self.inplanes, 1)\r\n",
        "        self.layer2 = self._make_residual(block, self.inplanes, 1)\r\n",
        "        self.layer3 = self._make_residual(block, self.num_feats, 1)\r\n",
        "        self.maxpool = nn.MaxPool2d(2, stride=2)\r\n",
        "\r\n",
        "        # build hourglass modules\r\n",
        "        ch = self.num_feats*block.expansion\r\n",
        "        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\r\n",
        "        for i in range(num_stacks):\r\n",
        "            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\r\n",
        "            res.append(self._make_residual(block, self.num_feats, num_blocks))\r\n",
        "            fc.append(self._make_fc(ch, ch))\r\n",
        "            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\r\n",
        "            if i < num_stacks-1:\r\n",
        "                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\r\n",
        "                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\r\n",
        "        self.hg = nn.ModuleList(hg)\r\n",
        "        self.res = nn.ModuleList(res)\r\n",
        "        self.fc = nn.ModuleList(fc)\r\n",
        "        self.score = nn.ModuleList(score)\r\n",
        "        self.fc_ = nn.ModuleList(fc_)\r\n",
        "        self.score_ = nn.ModuleList(score_)\r\n",
        "\r\n",
        "    def _make_residual(self, block, planes, blocks, stride=1):\r\n",
        "        downsample = None\r\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n",
        "                          kernel_size=1, stride=stride, bias=True),\r\n",
        "            )\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "        for i in range(1, blocks):\r\n",
        "            layers.append(block(self.inplanes, planes))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def _make_fc(self, inplanes, outplanes):\r\n",
        "        bn = nn.BatchNorm2d(inplanes)\r\n",
        "        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\r\n",
        "        return nn.Sequential(\r\n",
        "                conv,\r\n",
        "                bn,\r\n",
        "                self.relu,\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = []\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "\r\n",
        "        for i in range(self.num_stacks):\r\n",
        "            y = self.hg[i](x)\r\n",
        "            y = self.res[i](y)\r\n",
        "            y = self.fc[i](y)\r\n",
        "            score = self.score[i](y)\r\n",
        "            out.append(score)\r\n",
        "            if i < self.num_stacks-1:\r\n",
        "                fc_ = self.fc_[i](y)\r\n",
        "                score_ = self.score_[i](score)\r\n",
        "                x = x + fc_ + score_\r\n",
        "\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZw8Uolpbzn6"
      },
      "source": [
        "model = HourglassNet(Bottleneck, num_stacks=1, num_blocks=2, num_classes=22).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6aQQIGZxljN"
      },
      "source": [
        "----\r\n",
        "##### **>>> 4.2.2 Custom Body Landmark Dataset**\r\n",
        "해당 과제에서는 이미지 속 인물의 여러 신체 부위를 keypoint 형태로 예측하는 네트워크를 학습시키고자 합니다. (학습 시간 단축을 위해 일부 데이터만 사용)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ehOuP0seQg"
      },
      "source": [
        "과제를 수행하기 앞서 별도로 전달해드린 **데이터셋 활용 가이드**를 따라 **```APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip```** 압축 파일을 구글 드라이브에 **바로가기 추가**해주세요 :)\r\n",
        "\r\n",
        "\r\n",
        "<br></br>**주의!** 해당 과정은 **데이터 저작권 보호**를 위해 로컬로 직접 데이터를 <U>**다운로드 받는 것을 금지**</U>하기 때문입니다. 또한 교육이 종료된 이후에 해당 데이터셋을 **구글 드라이브에서 파기**할 것을 원칙으로 합니다.\r\n",
        "<img src='https://drive.google.com/uc?id=14_MWEN5u9mAvO1le_wYVPWdeduUnwW1B'  width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFBV7FShE4q-",
        "outputId": "6df9b0ae-adde-4bb9-be52-b545ffc1ce76"
      },
      "source": [
        "# Mount the google drive to access the dataset.\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGgSgaaBQ80i",
        "outputId": "0e402a16-09a2-4e85-8581-f55411c0ffd3"
      },
      "source": [
        "# 저장하신 압축 파일의 경로에 맞게 아래의 압축 해제 명령어를 수정해주세요. (!tar -zxvf 압축파일_경로 -C 저장할_폴더)\r\n",
        "\r\n",
        "!unzip /content/gdrive/MyDrive/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip -d /content/BodyLandmarkData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDPHjqgFE_3"
      },
      "source": [
        "# Hyper-paramter Settings\r\n",
        "data_root = '/content/BodyLandmarkData/data'\r\n",
        "log_dir   = '/content/BodyLandmarkData/log'\r\n",
        "\r\n",
        "epochs = 3\r\n",
        "batch_size = 8\r\n",
        "lr = 1e-3\r\n",
        "input_size = 320"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YzW4g_It3ix"
      },
      "source": [
        "학습을 위해서는 제공받은 데이터셋의 landmark 정보를 parsing하여 heatmap 형태로 나타내어야 합니다.\r\n",
        "\r\n",
        "Gaussin heatmap 형태로 keypoint를 나타내기 위하여 7강 강의 자료의 24번째 슬라이드를 참고하여 **TO DO**를 채워주세요 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x28F_kjzRyus"
      },
      "source": [
        "# Dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "class BodyLandmarkDataset(Dataset):\n",
        "  def __init__(self, data_root, is_Train=True, input_size=224, transform=None):\n",
        "    super(BodyLandmarkDataset, self).__init__()\n",
        "\n",
        "    self.img_list = self._load_img_list(data_root, is_Train)\n",
        "\n",
        "    self.len = len(self.img_list)\n",
        "    self.input_size = input_size\n",
        "    self.hm_size = input_size//4\n",
        "    self.transform = transform\n",
        "    \n",
        "    self.n_landmarks = 22\n",
        "    self.sigma = 1.5\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = self.img_list[index]\n",
        "    anno_path = img_path.replace('.jpg', '.json')\n",
        "    \n",
        "    # Image Loading\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img/255.\n",
        "    \n",
        "    org_size = img.shape[:2]\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    # Ground Truth\n",
        "    heatmap = self._get_heatmaps_from_json(anno_path, org_size)\n",
        "\n",
        "    return img, heatmap\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def _load_img_list(self, data_root, is_Train):\n",
        "    # Change the name of directory which has inconsistent naming rule.\n",
        "    full_img_list = glob(os.path.join(data_root, 'single', '*', '*color.jpg'))\n",
        "    \n",
        "    # ID < 400 for Training\n",
        "    # 400 < ID for Validation\n",
        "    if is_Train:\n",
        "      return [path for path in full_img_list if (self._load_img_ID(path) < 400)]\n",
        "    else:\n",
        "      return [path for path in full_img_list if (400 < self._load_img_ID(path))]\n",
        "\n",
        "  def _load_img_ID(self, path):\n",
        "    return int(path.split(os.sep)[-2].strip('id_1'))\n",
        "\n",
        "  def _get_heatmaps_from_json(self, anno_path, org_size):\n",
        "    # Parse point annotation\n",
        "    with open(anno_path, 'r') as json_file:\n",
        "      pts = json.load(json_file)\n",
        "    pts = np.array([(pt['pt_x'], pt['pt_y']) for pt in pts['DataList'][0]['coordinates']])\n",
        "\n",
        "    pts[:,0] = pts[:,0] / org_size[1] * self.hm_size\n",
        "    pts[:,1] = pts[:,1] / org_size[0] * self.hm_size\n",
        "\n",
        "    heatmap = np.zeros((self.n_landmarks, self.hm_size, self.hm_size), dtype=np.float32)\n",
        "    for i, pt in enumerate(pts):\n",
        "      heatmap[i] = self._draw_labelmap(heatmap[i], org_size, pt, self.sigma)\n",
        "    \n",
        "    return heatmap\n",
        "\n",
        "  def _draw_labelmap(self, heatmap, org_size, pt, sigma):\n",
        "    # Draw a 2D gaussian\n",
        "    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n",
        "    H, W = heatmap.shape[:2]\n",
        "\n",
        "    # Check that any part of the gaussian is in-bounds\n",
        "    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n",
        "    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n",
        "    if (ul[0] >= heatmap.shape[1] or ul[1] >= heatmap.shape[0] or\n",
        "            br[0] < 0 or br[1] < 0):\n",
        "        # If not, just return the image as is\n",
        "        return heatmap, 0\n",
        "\n",
        "    # Generate gaussian\n",
        "    size = 6 * sigma + 1\n",
        "    x = np.arange(0, size, 1, float)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    # The gaussian is not normalized, we want the center value to equal 1\n",
        "\n",
        "    '''======================================================='''\n",
        "    '''======================== TO DO ========================'''\n",
        "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "    '''======================== TO DO ========================'''\n",
        "    '''======================================================='''\n",
        "\n",
        "    # Usable gaussian range\n",
        "    g_x = max(0, -ul[0]), min(br[0], heatmap.shape[1]) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], heatmap.shape[0]) - ul[1]\n",
        "    # Image range\n",
        "    heatmap_x = max(0, ul[0]), min(br[0], heatmap.shape[1])\n",
        "    heatmap_y = max(0, ul[1]), min(br[1], heatmap.shape[0])\n",
        "\n",
        "    heatmap[heatmap_y[0]:heatmap_y[1], heatmap_x[0]:heatmap_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "    return heatmap\n",
        "    \n",
        "    return anno_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHBs5pAOWKoP"
      },
      "source": [
        "# Dataset and Data Loader\r\n",
        "MEAN = [0.485, 0.456, 0.406]\r\n",
        "STD  = [0.229, 0.224, 0.225]\r\n",
        "\r\n",
        "transform = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Resize((input_size, input_size)),\r\n",
        "    transforms.Normalize(mean=MEAN,\r\n",
        "                          std=STD)\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = BodyLandmarkDataset(data_root, is_Train=True, input_size=input_size, transform=transform)\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\r\n",
        "\r\n",
        "valid_dataset = BodyLandmarkDataset(data_root, is_Train=False, input_size=input_size, transform=transform)\r\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osvJGX4n4Miy"
      },
      "source": [
        "# Misc\r\n",
        "\r\n",
        "class AverageMeter(object):\r\n",
        "  \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "      self.reset()\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    self.val = 0\r\n",
        "    self.avg = 0\r\n",
        "    self.sum = 0\r\n",
        "    self.count = 0\r\n",
        "\r\n",
        "  def update(self, val, n=1):\r\n",
        "    self.val = val\r\n",
        "    self.sum += val * n\r\n",
        "    self.count += n\r\n",
        "    self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4edgVN4ovA0z"
      },
      "source": [
        "----\r\n",
        "#### **>>> 4.2.3 Training**\r\n",
        "\r\n",
        "```BodyLandmarkDataset```을 활용하여 Hourglass network를 학습할 시간입니다.\r\n",
        "\r\n",
        "- **TO DO Main (1)** : VGG-11을 본격적으로 학습하는 과정입니다. 주석에 적힌 내용을 따라 loss function인 ```criterion```과 ```optimizer```를 활용하여 빈 부분을 채워주세요.\r\n",
        "\r\n",
        "- **TO DO Main (2)** : 학습된 VGG-11을 validation dataset에 대해 평가하는 과정입니다. Validation 과정에서는 <U>gradient 계산과 backpropagation이 필요 없다</U>는 것에 주목하여 빈 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yewH3ejYbsG5"
      },
      "source": [
        "# Loss function and Optimizer\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7WybHwZbtfY",
        "outputId": "777a5b13-2111-43f8-a03d-4e1a168b6f56"
      },
      "source": [
        "# Main\r\n",
        "os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "with open(os.path.join(log_dir, 'train_log.csv'), 'w') as log:\r\n",
        "  for epoch in range(epochs):\r\n",
        "    train_loss, valid_loss = AverageMeter(), AverageMeter()\r\n",
        "\r\n",
        "    # Training\r\n",
        "    for iter, (img, hm_gt) in enumerate(train_loader):\r\n",
        "      '''================================================================'''\r\n",
        "      '''======================== TO DO Main (1) ========================'''\r\n",
        "      # optimizer에 저장된 미분값을 0으로 초기화\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "      # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "      img, hm_gt = img.float().cuda(), hm_gt.float().cuda()\r\n",
        "\r\n",
        "      # 모델에 이미지 forward\r\n",
        "      pred_logit = model(img)\r\n",
        "\r\n",
        "      # loss 값 계산\r\n",
        "      loss = 0\r\n",
        "      for pred in pred_logit:\r\n",
        "        loss += criterion(pred, hm_gt)\r\n",
        "\r\n",
        "      # Backpropagation\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      '''======================== TO DO Main (1) ========================'''\r\n",
        "      '''================================================================'''\r\n",
        "\r\n",
        "      # Log Update\r\n",
        "      train_loss.update(loss.item(), len(img))\r\n",
        "      print(\"\\rEpoch [%3d/%3d] | Iter [%3d/%3d] | Train Loss %.4f\" % (epoch+1, epochs, iter+1, len(train_loader), train_loss.avg), end='')\r\n",
        "\r\n",
        "    # Validation\r\n",
        "    for iter, (img, hm_gt) in enumerate(valid_loader):\r\n",
        "      '''================================================================'''\r\n",
        "      '''======================== TO DO Main (2) ========================'''\r\n",
        "      # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "      img, hm_gt = img.float().cuda(), hm_gt.float().cuda()\r\n",
        "\r\n",
        "      # 모델에 이미지 forward (gradient 계산 X)\r\n",
        "      with torch.no_grad():\r\n",
        "        pred_logit = model(img)\r\n",
        "\r\n",
        "      # loss 값 계산\r\n",
        "      loss = 0\r\n",
        "      for pred in pred_logit:\r\n",
        "        loss += criterion(pred, hm_gt)\r\n",
        "      '''======================== TO DO Main (2) ========================'''\r\n",
        "      '''================================================================'''\r\n",
        "\r\n",
        "      # Log Update\r\n",
        "      valid_loss.update(loss.item(), len(img))\r\n",
        " \r\n",
        "    print(\"\\nEpoch [%3d/%3d] | Valid Loss %.4f\" % (epoch+1, epochs, valid_loss.avg))\r\n",
        "    \r\n",
        "    # Log Writing\r\n",
        "    log.write('%d,%.4f,%.4f\\n'%(epoch, train_loss.avg, valid_loss.avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [  1/  3] | Iter [305/321] | Train Loss 0.0026"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3d21e6ea9044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhm_gt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;34m'''================================================================'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;34m'''======================== TO DO Main (1) ========================'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ae5ff7766dc1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Image Loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOCszlcvpoN"
      },
      "source": [
        "#### **>>> 4.3.3 Visualization**\r\n",
        "학습된 모델을 바탕으로 샘플 이미지에 대한 keypoint 예측 결과를 시각화하는 단계입니다.\r\n",
        "\r\n",
        "- **TO DO** : 아래의 시각화 코드를 활용하여 샘플 이미지에 대한 예측 결과를 시각화해주세요. ```matplotlib```을 이용하여 시각화한 그래프가 해당 colab notebook 파일에 남아있어야 하며 해당 과정을 마치신 뒤에 edwith의 댓글로 colab link를 남겨주세요. (아래 예시 그림 참고) <img src='https://drive.google.com/uc?id=1zCiRG-vQ2lSantORSmQYbqH75rB9Rb5f'  width=\"400\">\r\n",
        "\r\n",
        "\r\n",
        "- **TO DO Main** : 주석을 참고하여 inference를 위한 코드를 완성해주세요.\r\n",
        "\r\n",
        "- **TO DO Decoding** : 예측된 heatmap에서 좌표값 (x,y)를 얻어내는 코드를 완성해주세요.\r\n",
        "<br>(1) ```pred_hm``` 변수는 (channels, height, width) shape을 가집니다.\r\n",
        "<br>(2) ```hm``` 변수는 ```pred_hm```의 각 channel을 나타내며 (height, width) shape을 가집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1eoFBtv_U_ar"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "n_vis = 5\r\n",
        "\r\n",
        "# Visualize the result of validation dataset\r\n",
        "for iter, (imgs, hm_gts) in enumerate(train_loader):\r\n",
        "  '''============================================================'''\r\n",
        "  '''======================== TO DO Main ========================'''\r\n",
        "  # GPU 연산을 위해 이미지 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "  imgs = imgs.float().cuda()\r\n",
        "  \r\n",
        "  # 모델에 이미지 forward (gradient 계산 X)\r\n",
        "  with torch.no_grad():\r\n",
        "    preds = model(imgs)[-1].cpu().numpy()\r\n",
        "  '''======================== TO DO Main ========================'''\r\n",
        "  '''============================================================'''\r\n",
        "\r\n",
        "\r\n",
        "  # for each sample in a batch\r\n",
        "  imgs = imgs.cpu().numpy()\r\n",
        "  for img, pred_hm in zip(imgs, preds):\r\n",
        "    # Re-convert pre-processed input image to original format\r\n",
        "    img = np.moveaxis(img, 0, -1)\r\n",
        "    img = (img * STD) + MEAN\r\n",
        "    img = (img*255).astype(np.uint8).copy()\r\n",
        "\r\n",
        "    for hm in pred_hm:\r\n",
        "      '''======================================================='''\r\n",
        "      '''==================== TO DO Decoding ==================='''\r\n",
        "      y, x = np.where(hm == hm.max())\r\n",
        "      '''==================== TO DO Decoding ==================='''\r\n",
        "      '''======================================================='''\r\n",
        "      cv2.circle(img, (x[0]*4, y[0]*4), 3, (255,0,0), -1)\r\n",
        "    \r\n",
        "    plt.imshow(img)\r\n",
        "    plt.show()\r\n",
        "  \r\n",
        "\r\n",
        "  if iter == (n_vis-1):\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qkTj_oq100qL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}