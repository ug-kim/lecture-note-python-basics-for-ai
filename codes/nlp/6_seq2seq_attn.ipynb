{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_seq2seq_attn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**6. Seq2seq + Attention**\r\n",
        "1. 여러 Attention 모듈을 구현합니다.\r\n",
        "2. 기존 Seq2seq 모델과의 차이를 이해합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jx0cxjg2TZq",
        "outputId": "20105fbd-b071-4102-c8c4-40fc73f5030a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOoDGkaFkrd2"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from torch import nn\r\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "import torch\r\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz8nkrRZSysK"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DBRVAT32YEw"
      },
      "source": [
        "데이터 처리는 이전과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1neCRvux8k6Z"
      },
      "source": [
        "vocab_size = 100\r\n",
        "pad_id = 0\r\n",
        "sos_id = 1\r\n",
        "eos_id = 2\r\n",
        "\r\n",
        "src_data = [\r\n",
        "  [3, 77, 56, 26, 3, 55, 12, 36, 31],\r\n",
        "  [58, 20, 65, 46, 26, 10, 76, 44],\r\n",
        "  [58, 17, 8],\r\n",
        "  [59],\r\n",
        "  [29, 3, 52, 74, 73, 51, 39, 75, 19],\r\n",
        "  [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\r\n",
        "  [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\r\n",
        "  [75, 34, 17, 3, 86, 88],\r\n",
        "  [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\r\n",
        "  [12, 40, 69, 39, 49]\r\n",
        "]\r\n",
        "\r\n",
        "trg_data = [\r\n",
        "  [75, 13, 22, 77, 89, 21, 13, 86, 95],\r\n",
        "  [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\r\n",
        "  [85, 8, 50, 30],\r\n",
        "  [47, 30],\r\n",
        "  [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\r\n",
        "  [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\r\n",
        "  [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\r\n",
        "  [16, 98, 68, 57, 55, 46, 66, 85, 18],\r\n",
        "  [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\r\n",
        "  [37, 93, 98, 13, 45, 28, 89, 72, 70]\r\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwq5SNGUdCT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0445fb-44da-40ad-d67f-b0060e34df91"
      },
      "source": [
        "trg_data = [[sos_id]+seq+[eos_id] for seq in tqdm(trg_data)]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 11450.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSeExSrRAYg8"
      },
      "source": [
        "def padding(data, is_src=True):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  valid_lens = []\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    valid_lens.append(len(seq))\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, valid_lens, max_len"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCXaXdk-ApJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ba962d-bf31-4b7a-a316-8c9a2ac2ab2a"
      },
      "source": [
        "src_data, src_lens, src_max_len = padding(src_data)\r\n",
        "trg_data, trg_lens, trg_max_len = padding(trg_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 3871.78it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 68985.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 15\n",
            "Maximum sequence length: 22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F3Mx8pbAvqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdb0877-6cf7-418e-ac52-b446f1ef6c32"
      },
      "source": [
        "# B: batch size, S_L: source maximum sequence length, T_L: target maximum sequence length\r\n",
        "src_batch = torch.LongTensor(src_data)  # (B, S_L)\r\n",
        "src_batch_lens = torch.LongTensor(src_lens)  # (B)\r\n",
        "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\r\n",
        "trg_batch_lens = torch.LongTensor(trg_lens)  # (B)\r\n",
        "\r\n",
        "print(src_batch.shape)\r\n",
        "print(src_batch_lens.shape)\r\n",
        "print(trg_batch.shape)\r\n",
        "print(trg_batch_lens.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 15])\n",
            "torch.Size([10])\n",
            "torch.Size([10, 22])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxmvrpQABWn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909730b4-36c6-47be-8cc9-b52eb05244b7"
      },
      "source": [
        "src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\r\n",
        "src_batch = src_batch[sorted_idx]\r\n",
        "trg_batch = trg_batch[sorted_idx]\r\n",
        "trg_batch_lens = trg_batch_lens[sorted_idx]\r\n",
        "\r\n",
        "print(src_batch)\r\n",
        "print(src_batch_lens)\r\n",
        "print(trg_batch)\r\n",
        "print(trg_batch_lens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n",
            "        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n",
            "        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n",
            "        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n",
            "        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n",
            "        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n",
            "tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n",
            "         71, 69, 88,  2],\n",
            "        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0]])\n",
            "tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emD3bFjS2vEn"
      },
      "source": [
        "### **Encoder 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5k9sSui29yP"
      },
      "source": [
        "Encoder 역시 기존 Seq2seq 모델과 동일합니다.\r\n",
        "- 다른 점은 마지막 hidden state vector만 linear transformation을 해줬다면, attention을 해주기 위해 각 time step마다 뽑은 hidden state vector들인 output도 linear transformation 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmhCME-PDUJ8"
      },
      "source": [
        "embedding_size = 256\r\n",
        "hidden_size = 512\r\n",
        "num_layers = 2\r\n",
        "num_dirs = 2\r\n",
        "dropout = 0.1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epZDaDO-FMPu"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = nn.GRU(\r\n",
        "        input_size=embedding_size, \r\n",
        "        hidden_size=hidden_size,\r\n",
        "        num_layers=num_layers,\r\n",
        "        bidirectional=True if num_dirs > 1 else False,\r\n",
        "        dropout=dropout\r\n",
        "    )\r\n",
        "    self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\r\n",
        "\r\n",
        "  def forward(self, batch, batch_lens):  # batch: (B, S_L), batch_lens: (B)\r\n",
        "    # d_w: word embedding size\r\n",
        "    batch_emb = self.embedding(batch)  # (B, S_L, d_w)\r\n",
        "    batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)\r\n",
        "\r\n",
        "    packed_input = pack_padded_sequence(batch_emb, batch_lens)\r\n",
        "\r\n",
        "    h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)\r\n",
        "    packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)\r\n",
        "    outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)\r\n",
        "    outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)\r\n",
        "\r\n",
        "    forward_hidden = h_n[-2, :, :]\r\n",
        "    backward_hidden = h_n[-1, :, :]\r\n",
        "    hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)\r\n",
        "\r\n",
        "    return outputs, hidden"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEdSnKZkIedk"
      },
      "source": [
        "encoder = Encoder()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w5G0uy4TiFA"
      },
      "source": [
        "### **Dot-product Attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-sPMEBEcRqP"
      },
      "source": [
        "우선 대표적인 attention 형태 중 하나인 Dot-product Attention은 다음과 같이 구현할 수 있습니다.\r\n",
        "\r\n",
        "- Attention: decoder에서 각 단어를 예측할 때 수행된다\r\n",
        "- Attention을 decoder에 ad-one? 모듈로 추가\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEB-og7IcYN6"
      },
      "source": [
        "class DotAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\r\n",
        "    query = decoder_hidden.squeeze(0)  # (B, d_h)\r\n",
        "    key = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    # 내적\r\n",
        "    # query를 길이 차원에서 unsqueeze\r\n",
        "    # 그리고 mul 해주면, 각 encoder hidden state 길이만큼 반복적으로 각 차원이 곱해지고\r\n",
        "    # 그것을 다 sum 해주면 내적 연산과 같다\r\n",
        "    energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1)  # (B, S_L)\r\n",
        "\r\n",
        "    attn_scores = F.softmax(energy, dim=-1)  # (B, S_L)\r\n",
        "    # encoder hidden state에 적용되는 가중치로 attn_scores 사용\r\n",
        "    # attn_scores.unsqueeze(2) - (B, S_L, 1)\r\n",
        "    # encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\r\n",
        "    attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1)  # (B, d_h)\r\n",
        "\r\n",
        "    return attn_values, attn_scores"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIARwx4IjuuG"
      },
      "source": [
        "dot_attn = DotAttention()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94WCkbCjMnz"
      },
      "source": [
        "이제 이 attention 모듈을 가지는 Decoder 클래스를 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JycRs0ojLyg"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, attention):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.attention = attention\r\n",
        "    self.rnn = nn.GRU(\r\n",
        "        embedding_size,\r\n",
        "        hidden_size\r\n",
        "    )\r\n",
        "    self.output_linear = nn.Linear(2*hidden_size, vocab_size)\r\n",
        "\r\n",
        "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (L, B, d_h), hidden: (1, B, d_h)  \r\n",
        "    batch_emb = self.embedding(batch)  # (B, d_w)\r\n",
        "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\r\n",
        "\r\n",
        "    outputs, hidden = self.rnn(batch_emb, hidden)  # (1, B, d_h), (1, B, d_h)\r\n",
        "\r\n",
        "    # attn_values - context vector\r\n",
        "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\r\n",
        "    concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1)  # (1, B, 2d_h)\r\n",
        "\r\n",
        "    return self.output_linear(concat_outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45GG2CvOjwzE"
      },
      "source": [
        "decoder = Decoder(dot_attn)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ1NzYZROrOu"
      },
      "source": [
        "### **Seq2seq 모델 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYvsQS0Ovp6"
      },
      "source": [
        "최종적으로 seq2seq 모델을 다음과 같이 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52xKNVeF37N"
      },
      "source": [
        "class Seq2seq(nn.Module):\r\n",
        "  def __init__(self, encoder, decoder):\r\n",
        "    super(Seq2seq, self).__init__()\r\n",
        "\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "\r\n",
        "  def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\r\n",
        "    # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)\r\n",
        "\r\n",
        "    encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens)  # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\r\n",
        "\r\n",
        "    input_ids = trg_batch[:, 0]  # (B)\r\n",
        "    batch_size = src_batch.shape[0]\r\n",
        "    outputs = torch.zeros(trg_max_len, batch_size, vocab_size)  # (T_L, B, V)\r\n",
        "\r\n",
        "    for t in range(1, trg_max_len):\r\n",
        "      # decoder에 encoder_outputs를 넣어줘야 한다\r\n",
        "      decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)  # decoder_outputs: (B, V), hidden: (1, B, d_h)\r\n",
        "\r\n",
        "      outputs[t] = decoder_outputs\r\n",
        "      _, top_ids = torch.max(decoder_outputs, dim=-1)  # top_ids: (B)\r\n",
        "\r\n",
        "      # teacher forcing\r\n",
        "      input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\r\n",
        "\r\n",
        "    return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNv7wlRgPIYS"
      },
      "source": [
        "seq2seq = Seq2seq(encoder, decoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFwbnxd7PVNf"
      },
      "source": [
        "### **모델 사용해보기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIprc5N2jaV2"
      },
      "source": [
        "만든 모델로 결과를 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKdTDHqsiLbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde388a5-3d40-4bc6-991f-76f7cfb013a8"
      },
      "source": [
        "# V: vocab size\r\n",
        "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)  # (T_L, B, V)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.2008,  0.0702, -0.0005,  ..., -0.0186, -0.0342, -0.0736],\n",
            "         [ 0.1734,  0.0434, -0.0743,  ..., -0.0206, -0.0342, -0.0497],\n",
            "         [ 0.1761,  0.0771, -0.0645,  ...,  0.0041, -0.0357, -0.0523],\n",
            "         ...,\n",
            "         [ 0.2016,  0.0640, -0.0526,  ...,  0.0229, -0.0728, -0.0669],\n",
            "         [ 0.1554,  0.0802, -0.0775,  ..., -0.0300, -0.0106, -0.0746],\n",
            "         [ 0.1969,  0.0566, -0.0552,  ..., -0.0302, -0.0343, -0.0793]],\n",
            "\n",
            "        [[-0.0208, -0.0655, -0.1035,  ...,  0.0093, -0.0087, -0.1639],\n",
            "         [-0.0378, -0.0863, -0.1448,  ..., -0.0045,  0.0055, -0.1594],\n",
            "         [-0.0125,  0.0704, -0.1447,  ..., -0.0214, -0.0269, -0.1457],\n",
            "         ...,\n",
            "         [-0.0036,  0.0571, -0.1394,  ..., -0.0146, -0.0351, -0.1573],\n",
            "         [-0.0293,  0.0617, -0.1540,  ..., -0.0487, -0.0044, -0.1691],\n",
            "         [-0.0089,  0.0488, -0.1423,  ..., -0.0511, -0.0124, -0.1690]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0994,  0.0737, -0.1525,  ..., -0.0152,  0.0302, -0.1065],\n",
            "         [ 0.1451,  0.0602,  0.0194,  ..., -0.1278,  0.0360,  0.1870],\n",
            "         [ 0.1777,  0.0812,  0.0220,  ..., -0.1044,  0.0078,  0.2127],\n",
            "         ...,\n",
            "         [ 0.1209,  0.0695, -0.1535,  ...,  0.0045,  0.0404, -0.0908],\n",
            "         [-0.0148,  0.0501, -0.0891,  ...,  0.1535,  0.0639, -0.1885],\n",
            "         [-0.0120,  0.0481, -0.0862,  ...,  0.1527,  0.0673, -0.1887]],\n",
            "\n",
            "        [[ 0.0560,  0.0728, -0.1121,  ..., -0.1172, -0.0225, -0.1175],\n",
            "         [ 0.0730,  0.0480,  0.0110,  ..., -0.1989,  0.0109,  0.0532],\n",
            "         [ 0.1043,  0.0712,  0.0143,  ..., -0.1736, -0.0189,  0.0758],\n",
            "         ...,\n",
            "         [ 0.0782,  0.0713, -0.1151,  ..., -0.0992, -0.0159, -0.1014],\n",
            "         [-0.0109,  0.0577, -0.0762,  ..., -0.0210,  0.0042, -0.1179],\n",
            "         [-0.0076,  0.0555, -0.0735,  ..., -0.0216,  0.0077, -0.1181]],\n",
            "\n",
            "        [[ 0.0856,  0.0756, -0.1529,  ..., -0.0390,  0.0261, -0.1258],\n",
            "         [ 0.1410,  0.0498,  0.0199,  ..., -0.1538,  0.0381,  0.1554],\n",
            "         [ 0.1700,  0.0734,  0.0224,  ..., -0.1288,  0.0109,  0.1766],\n",
            "         ...,\n",
            "         [ 0.1074,  0.0701, -0.1547,  ..., -0.0200,  0.0338, -0.1104],\n",
            "         [ 0.0505,  0.0746, -0.1417,  ...,  0.0066,  0.0453, -0.1030],\n",
            "         [ 0.0556,  0.0707, -0.1381,  ...,  0.0060,  0.0501, -0.1035]]],\n",
            "       grad_fn=<CopySlices>)\n",
            "torch.Size([22, 10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-wAEwi9dy0Q"
      },
      "source": [
        "sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\r\n",
        "sample_len = len(sample_sent)\r\n",
        "\r\n",
        "sample_batch = torch.LongTensor(sample_sent).unsqueeze(0)  # (1, L)\r\n",
        "sample_batch_len = torch.LongTensor([sample_len])  # (1)\r\n",
        "\r\n",
        "encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len)  # hidden: (4, 1, d_h)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ywRSK1iTn1U"
      },
      "source": [
        "input_id = torch.LongTensor([sos_id]) # (1)\r\n",
        "output = []\r\n",
        "\r\n",
        "for t in range(1, trg_max_len):\r\n",
        "  decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)  # decoder_output: (1, V), hidden: (4, 1, d_h)\r\n",
        "\r\n",
        "  _, top_id = torch.max(decoder_output, dim=-1)  # top_ids: (1)\r\n",
        "\r\n",
        "  if top_id == eos_id:\r\n",
        "    break\r\n",
        "  else:\r\n",
        "    output += top_id.tolist()\r\n",
        "    input_id = top_id"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_A4ZrhTXik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077b9d3f-0c04-45d8-96cf-8ec35a1f7316"
      },
      "source": [
        "output"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[87,\n",
              " 10,\n",
              " 86,\n",
              " 63,\n",
              " 79,\n",
              " 61,\n",
              " 59,\n",
              " 1,\n",
              " 87,\n",
              " 10,\n",
              " 86,\n",
              " 63,\n",
              " 79,\n",
              " 61,\n",
              " 59,\n",
              " 1,\n",
              " 87,\n",
              " 10,\n",
              " 86,\n",
              " 63,\n",
              " 79]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4TZfceq3Nbs"
      },
      "source": [
        "### **Concat Attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxpAQjm3Y9U"
      },
      "source": [
        "Bahdanau Attention이라고도 불리는 Concat Attention을 구현해보도록 하겠습니다.  \r\n",
        "\r\n",
        "\r\n",
        "*   `self.w`: Concat한 query와 key 벡터를 1차적으로 linear transformation.\r\n",
        "*   `self.v`: Attention logit 값을 계산.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE0GvGpsnx3I"
      },
      "source": [
        "해당 시점의 decoder hidden vector, encoder hidden vectors를 concat해 특정 layer를 통과시켜 score를 계산하겠다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHRfFeIzJJU7"
      },
      "source": [
        "class ConcatAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.w = nn.Linear(2*hidden_size, hidden_size, bias=False)\r\n",
        "    self.v = nn.Linear(hidden_size, 1, bias=False)  # to scalar score\r\n",
        "\r\n",
        "  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\r\n",
        "    src_max_len = encoder_outputs.shape[0]\r\n",
        "\r\n",
        "    decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1)  # (B, S_L, d_h)\r\n",
        "    encoder_outputs = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # (B, S_L, 2d_h)\r\n",
        "    energy = torch.tanh(self.w(concat_hiddens))  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    attn_scores = F.softmax(self.v(energy), dim=1)  # (B, S_L, 1)\r\n",
        "    attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1)  # (B, d_h)\r\n",
        "\r\n",
        "    return attn_values, attn_scores"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utm4b5uzNS40"
      },
      "source": [
        "concat_attn = ConcatAttention()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBCV9G-M1cw"
      },
      "source": [
        "마찬가지로 decoder를 마저 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnppmsXNSaGP"
      },
      "source": [
        "# 외부적으로 for문 돌아준다\r\n",
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, attention):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.attention = attention\r\n",
        "    self.rnn = nn.GRU(\r\n",
        "        embedding_size + hidden_size,  # here is concat\r\n",
        "        hidden_size\r\n",
        "    )\r\n",
        "    self.output_linear = nn.Linear(hidden_size, vocab_size)\r\n",
        "\r\n",
        "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)  \r\n",
        "    batch_emb = self.embedding(batch)  # (B, d_w)\r\n",
        "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\r\n",
        "\r\n",
        "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\r\n",
        "\r\n",
        "    concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1)  # (1, B, d_w+d_h)\r\n",
        "\r\n",
        "    outputs, hidden = self.rnn(concat_emb, hidden)  # (1, B, d_h), (1, B, d_h)\r\n",
        "\r\n",
        "    return self.output_linear(outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gA4GJqgOoMT"
      },
      "source": [
        "decoder = Decoder(concat_attn)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQI9J0VGj4cc"
      },
      "source": [
        "seq2seq = Seq2seq(encoder, decoder)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6suDtTxj4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a4be87-ba10-413d-8ddd-5cbcf4f63f40"
      },
      "source": [
        "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-2.2273e-01,  2.3595e-02,  6.0788e-02,  ..., -1.8632e-02,\n",
            "          -1.1531e-01, -7.5364e-02],\n",
            "         [-1.7305e-01, -6.1007e-03,  7.7531e-02,  ..., -2.7252e-02,\n",
            "          -9.5774e-02,  1.9819e-04],\n",
            "         [-1.6265e-01,  4.5944e-03,  6.3686e-02,  ...,  4.9647e-02,\n",
            "          -1.0497e-01, -3.6956e-02],\n",
            "         ...,\n",
            "         [-1.7146e-01,  3.5991e-02,  6.0810e-02,  ..., -2.1611e-02,\n",
            "          -3.0836e-02, -2.4390e-02],\n",
            "         [-1.6247e-01,  4.1773e-02,  5.1392e-02,  ..., -6.1773e-02,\n",
            "          -4.7402e-02, -1.9630e-02],\n",
            "         [-2.1087e-01,  1.3589e-02,  4.2362e-02,  ..., -2.7162e-02,\n",
            "          -7.6996e-02, -4.0088e-02]],\n",
            "\n",
            "        [[-1.0010e-01, -4.6199e-02,  1.4477e-01,  ...,  1.8558e-01,\n",
            "          -1.9937e-01, -9.7453e-02],\n",
            "         [-5.4948e-02, -7.1771e-02,  1.4864e-01,  ...,  1.9075e-01,\n",
            "          -1.8016e-01, -6.5030e-02],\n",
            "         [-5.1427e-02, -6.1454e-02,  1.4495e-01,  ...,  2.2660e-01,\n",
            "          -1.8368e-01, -8.1619e-02],\n",
            "         ...,\n",
            "         [-5.5808e-02, -4.7133e-02,  1.4618e-01,  ...,  1.8403e-01,\n",
            "          -1.5056e-01, -7.4659e-02],\n",
            "         [-5.6163e-02, -4.3194e-02,  1.3874e-01,  ...,  1.6158e-01,\n",
            "          -1.5167e-01, -7.4350e-02],\n",
            "         [-8.3085e-02, -6.0442e-02,  1.3636e-01,  ...,  1.8760e-01,\n",
            "          -1.7129e-01, -8.4507e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-5.5954e-02,  3.8702e-02,  1.4341e-01,  ...,  3.1698e-02,\n",
            "          -1.1598e-01, -2.8914e-01],\n",
            "         [-4.3701e-02,  1.5096e-02,  1.2225e-01,  ...,  9.4519e-02,\n",
            "          -1.7830e-01, -2.5466e-01],\n",
            "         [-5.4928e-02,  3.9508e-02,  1.4638e-01,  ...,  8.6020e-02,\n",
            "          -1.7456e-01, -2.5335e-01],\n",
            "         ...,\n",
            "         [-3.9432e-02,  2.8886e-02,  1.4982e-01,  ...,  8.2014e-02,\n",
            "          -1.8344e-01, -2.5435e-01],\n",
            "         [-4.6937e-02,  2.8143e-02,  1.4635e-01,  ...,  8.2976e-02,\n",
            "          -1.9035e-01, -2.5119e-01],\n",
            "         [-4.5329e-02,  2.8881e-02,  1.4379e-01,  ...,  7.6730e-02,\n",
            "          -1.7063e-01, -2.5462e-01]],\n",
            "\n",
            "        [[ 5.4163e-02, -6.8968e-03, -1.6130e-02,  ...,  2.1478e-02,\n",
            "          -2.5502e-01, -1.7566e-01],\n",
            "         [ 7.3718e-02, -3.1493e-02, -2.6336e-02,  ...,  7.2226e-02,\n",
            "          -2.8320e-01, -1.5366e-01],\n",
            "         [-4.8622e-02,  9.4183e-04,  5.3375e-02,  ...,  1.5580e-02,\n",
            "          -4.8673e-02, -1.8697e-01],\n",
            "         ...,\n",
            "         [-3.4011e-02, -1.5451e-02,  5.8613e-02,  ...,  1.7907e-02,\n",
            "          -4.4948e-02, -1.8108e-01],\n",
            "         [-4.3243e-02, -1.3948e-02,  5.3077e-02,  ...,  1.9348e-02,\n",
            "          -4.8398e-02, -1.7984e-01],\n",
            "         [ 7.6107e-02, -2.8813e-02, -8.6103e-03,  ...,  5.9302e-02,\n",
            "          -2.8095e-01, -1.5625e-01]],\n",
            "\n",
            "        [[ 1.5763e-02, -7.2493e-02,  7.1959e-02,  ..., -5.2171e-02,\n",
            "          -1.5211e-01, -1.9213e-01],\n",
            "         [ 4.3427e-02, -9.1905e-02,  6.8563e-02,  ..., -1.1755e-02,\n",
            "          -1.6046e-01, -1.7843e-01],\n",
            "         [ 1.1319e-02, -6.2201e-02,  7.5098e-02,  ..., -5.0849e-02,\n",
            "          -1.8944e-02, -1.9549e-01],\n",
            "         ...,\n",
            "         [ 2.8853e-02, -7.4526e-02,  8.3417e-02,  ..., -4.4536e-02,\n",
            "          -1.2915e-02, -1.9188e-01],\n",
            "         [ 1.8597e-02, -7.4016e-02,  7.6428e-02,  ..., -4.3573e-02,\n",
            "          -1.5130e-02, -1.9250e-01],\n",
            "         [ 4.6834e-02, -9.2477e-02,  8.2508e-02,  ..., -2.2681e-02,\n",
            "          -1.5869e-01, -1.7842e-01]]], grad_fn=<CopySlices>)\n",
            "torch.Size([22, 10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXuV9FljpQmm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}