{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_multi_head_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**7. Multi-head Attention**\r\n",
        "1. Multi-head attention 및 self-attention 구현.\r\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해.\r\n",
        "- 여기는 Encoder 부분이다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJK1tn-V0Hnn",
        "outputId": "b3aee1e3-8fa2-47b5-e721-8e6ff1535f7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "# RNN이 아니기 때문에 packed object 필요 없다\r\n",
        "\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\r\n",
        "  [75, 51],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [21, 39, 10, 64, 21],\r\n",
        "  [98],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\r\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\r\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해\r\n",
        "# valid_len 계산할 필요 없다\r\n",
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4467b1d0-9e2e-4994-d6db-b9f3c10b20e5"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 2848.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3145ebad-1063-456c-db95-c9179b7ea512"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\r\n",
        "num_heads = 8  # head의 개수\r\n",
        "\r\n",
        "# d_model이 num_heads로 나누어 떨어져야 한다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a028a445-0182-4ec3-85d6-5586fe91375c"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.5375,  1.4314, -0.0752,  ..., -0.8263,  0.8717,  0.7618],\n",
            "         [ 0.5835, -1.8638,  0.4799,  ...,  0.0206,  0.0927, -0.8794],\n",
            "         [ 1.0635,  0.3856, -0.0417,  ..., -0.0522, -0.4558,  0.8001],\n",
            "         ...,\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225]],\n",
            "\n",
            "        [[ 1.7256,  1.4002,  2.0348,  ...,  0.9457,  0.1526,  0.3471],\n",
            "         [-2.3295,  0.6252,  0.7374,  ..., -0.6947, -0.6032, -1.8170],\n",
            "         [-0.3749,  0.3878, -1.6112,  ...,  0.8582, -0.5425, -0.8985],\n",
            "         ...,\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225]],\n",
            "\n",
            "        [[-1.0352, -1.7133,  1.2455,  ..., -0.3931, -0.0878, -1.2596],\n",
            "         [ 1.0222,  0.0187, -0.3459,  ..., -0.8294, -0.2097, -0.6768],\n",
            "         [-0.9040, -0.1241, -0.6967,  ..., -0.4027, -1.0015, -1.3815],\n",
            "         ...,\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.5972,  0.1171, -0.0038,  ..., -2.1488, -0.3542, -1.6201],\n",
            "         [ 0.2071, -0.2193,  1.3240,  ...,  1.2237,  0.7420, -0.9718],\n",
            "         [-0.3749,  0.3878, -1.6112,  ...,  0.8582, -0.5425, -0.8985],\n",
            "         ...,\n",
            "         [ 0.2986,  0.9592,  0.6584,  ...,  0.5153, -1.3851, -0.7773],\n",
            "         [ 1.8104, -0.3636, -1.5459,  ..., -0.0946, -1.2940, -0.9665],\n",
            "         [ 1.8099, -0.4773, -1.6640,  ...,  0.8191,  1.1549, -0.6586]],\n",
            "\n",
            "        [[ 0.0588,  0.1373, -1.5743,  ..., -0.5140, -0.2780,  2.1257],\n",
            "         [ 1.4519,  0.6633, -1.5346,  ..., -0.0158, -1.0976, -0.2307],\n",
            "         [-0.4605, -0.0600, -0.3783,  ...,  0.1628, -0.4375, -2.0351],\n",
            "         ...,\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225]],\n",
            "\n",
            "        [[ 2.9108, -0.3771,  0.6382,  ...,  0.1280, -0.7803, -1.1036],\n",
            "         [ 1.4519,  0.6633, -1.5346,  ..., -0.0158, -1.0976, -0.2307],\n",
            "         [ 0.5005,  1.7956,  1.1286,  ..., -0.8516,  1.2687, -0.8508],\n",
            "         ...,\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225],\n",
            "         [-0.0265,  2.4331,  1.2265,  ..., -1.1198,  0.2593, -0.2225]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1707f866-ed4f-4b87-89b8-651fe4d29ac7"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\r\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\r\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22f50b5-2830-48fe-8308-ce0ac49f65dc"
      },
      "source": [
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "# num_heads * d_k로 쪼갠다\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12751a52-14d7-405d-bf6f-a894e972dd10"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\r\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Scaled dot-product self-attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec35d872-53b5-46a0-c3eb-fefde5a2a892"
      },
      "source": [
        "# shape - (L, L)\r\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\r\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "# softmax - row-wise이기 때문에 dim은 -1\r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.0499, 0.0405, 0.0438,  ..., 0.0613, 0.0613, 0.0613],\n",
            "          [0.0447, 0.0532, 0.0307,  ..., 0.0315, 0.0315, 0.0315],\n",
            "          [0.0713, 0.0617, 0.0347,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          ...,\n",
            "          [0.0584, 0.0449, 0.0351,  ..., 0.0216, 0.0216, 0.0216],\n",
            "          [0.0584, 0.0449, 0.0351,  ..., 0.0216, 0.0216, 0.0216],\n",
            "          [0.0584, 0.0449, 0.0351,  ..., 0.0216, 0.0216, 0.0216]],\n",
            "\n",
            "         [[0.0438, 0.0642, 0.0674,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0508, 0.0594, 0.0724,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          [0.0830, 0.0343, 0.0587,  ..., 0.0384, 0.0384, 0.0384],\n",
            "          ...,\n",
            "          [0.0281, 0.0449, 0.0280,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          [0.0281, 0.0449, 0.0280,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          [0.0281, 0.0449, 0.0280,  ..., 0.0525, 0.0525, 0.0525]],\n",
            "\n",
            "         [[0.0351, 0.0461, 0.0615,  ..., 0.0352, 0.0352, 0.0352],\n",
            "          [0.0337, 0.0531, 0.0490,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0294, 0.0423, 0.0742,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          ...,\n",
            "          [0.0594, 0.0661, 0.0649,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0594, 0.0661, 0.0649,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0594, 0.0661, 0.0649,  ..., 0.0515, 0.0515, 0.0515]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0501, 0.0305, 0.0353,  ..., 0.0673, 0.0673, 0.0673],\n",
            "          [0.0689, 0.0423, 0.0624,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0782, 0.0352, 0.0592,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          ...,\n",
            "          [0.0251, 0.0389, 0.0491,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0251, 0.0389, 0.0491,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0251, 0.0389, 0.0491,  ..., 0.0599, 0.0599, 0.0599]],\n",
            "\n",
            "         [[0.0431, 0.0502, 0.0493,  ..., 0.0636, 0.0636, 0.0636],\n",
            "          [0.0438, 0.0460, 0.0473,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0565, 0.0816, 0.0552,  ..., 0.0395, 0.0395, 0.0395],\n",
            "          ...,\n",
            "          [0.0564, 0.0454, 0.0455,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0564, 0.0454, 0.0455,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0564, 0.0454, 0.0455,  ..., 0.0496, 0.0496, 0.0496]],\n",
            "\n",
            "         [[0.0398, 0.0433, 0.0431,  ..., 0.0833, 0.0833, 0.0833],\n",
            "          [0.0597, 0.0713, 0.0236,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          [0.0632, 0.0598, 0.0332,  ..., 0.0285, 0.0285, 0.0285],\n",
            "          ...,\n",
            "          [0.0513, 0.0520, 0.0423,  ..., 0.0380, 0.0380, 0.0380],\n",
            "          [0.0513, 0.0520, 0.0423,  ..., 0.0380, 0.0380, 0.0380],\n",
            "          [0.0513, 0.0520, 0.0423,  ..., 0.0380, 0.0380, 0.0380]]],\n",
            "\n",
            "\n",
            "        [[[0.0731, 0.0579, 0.0358,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0410, 0.0461, 0.0922,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          [0.0588, 0.0363, 0.0797,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          ...,\n",
            "          [0.1122, 0.0674, 0.1068,  ..., 0.0378, 0.0378, 0.0378],\n",
            "          [0.1122, 0.0674, 0.1068,  ..., 0.0378, 0.0378, 0.0378],\n",
            "          [0.1122, 0.0674, 0.1068,  ..., 0.0378, 0.0378, 0.0378]],\n",
            "\n",
            "         [[0.0879, 0.0440, 0.0546,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0367, 0.0186, 0.0442,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0384, 0.0665, 0.0276,  ..., 0.0544, 0.0544, 0.0544],\n",
            "          ...,\n",
            "          [0.0379, 0.0878, 0.0336,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0379, 0.0878, 0.0336,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0379, 0.0878, 0.0336,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         [[0.0225, 0.0241, 0.0214,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          [0.0280, 0.0228, 0.0973,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0614, 0.0549, 0.0325,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          ...,\n",
            "          [0.0477, 0.0333, 0.0833,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0477, 0.0333, 0.0833,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0477, 0.0333, 0.0833,  ..., 0.0493, 0.0493, 0.0493]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0979, 0.0438, 0.0510,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0812, 0.0841, 0.0728,  ..., 0.0418, 0.0418, 0.0418],\n",
            "          [0.0670, 0.0597, 0.0506,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          ...,\n",
            "          [0.0691, 0.0660, 0.0520,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0691, 0.0660, 0.0520,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0691, 0.0660, 0.0520,  ..., 0.0491, 0.0491, 0.0491]],\n",
            "\n",
            "         [[0.0311, 0.1162, 0.0315,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.1055, 0.0636, 0.0695,  ..., 0.0392, 0.0392, 0.0392],\n",
            "          [0.0622, 0.0257, 0.0602,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          ...,\n",
            "          [0.0576, 0.0693, 0.0338,  ..., 0.0460, 0.0460, 0.0460],\n",
            "          [0.0576, 0.0693, 0.0338,  ..., 0.0460, 0.0460, 0.0460],\n",
            "          [0.0576, 0.0693, 0.0338,  ..., 0.0460, 0.0460, 0.0460]],\n",
            "\n",
            "         [[0.0438, 0.0263, 0.0209,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0489, 0.0625, 0.0404,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          [0.0446, 0.0435, 0.0464,  ..., 0.0523, 0.0523, 0.0523],\n",
            "          ...,\n",
            "          [0.0243, 0.0819, 0.0512,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0243, 0.0819, 0.0512,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0243, 0.0819, 0.0512,  ..., 0.0504, 0.0504, 0.0504]]],\n",
            "\n",
            "\n",
            "        [[[0.0476, 0.0306, 0.0591,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0382, 0.0332, 0.0483,  ..., 0.0547, 0.0547, 0.0547],\n",
            "          [0.0344, 0.0459, 0.0321,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          ...,\n",
            "          [0.0752, 0.0724, 0.0779,  ..., 0.0275, 0.0275, 0.0275],\n",
            "          [0.0752, 0.0724, 0.0779,  ..., 0.0275, 0.0275, 0.0275],\n",
            "          [0.0752, 0.0724, 0.0779,  ..., 0.0275, 0.0275, 0.0275]],\n",
            "\n",
            "         [[0.0429, 0.0359, 0.0661,  ..., 0.0425, 0.0425, 0.0425],\n",
            "          [0.0502, 0.0464, 0.0562,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0411, 0.0548, 0.0251,  ..., 0.0593, 0.0593, 0.0593],\n",
            "          ...,\n",
            "          [0.0733, 0.0438, 0.0551,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0733, 0.0438, 0.0551,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0733, 0.0438, 0.0551,  ..., 0.0511, 0.0511, 0.0511]],\n",
            "\n",
            "         [[0.0562, 0.0267, 0.0320,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0575, 0.0490, 0.0464,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0446, 0.0840, 0.0411,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          ...,\n",
            "          [0.0338, 0.0589, 0.0433,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0338, 0.0589, 0.0433,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0338, 0.0589, 0.0433,  ..., 0.0501, 0.0501, 0.0501]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0872, 0.0784, 0.0507,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0525, 0.0244, 0.0389,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0273, 0.0371, 0.0213,  ..., 0.0685, 0.0685, 0.0685],\n",
            "          ...,\n",
            "          [0.0723, 0.0570, 0.0404,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0723, 0.0570, 0.0404,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0723, 0.0570, 0.0404,  ..., 0.0600, 0.0600, 0.0600]],\n",
            "\n",
            "         [[0.0371, 0.0708, 0.0292,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0474, 0.0368, 0.0364,  ..., 0.0610, 0.0610, 0.0610],\n",
            "          [0.0549, 0.0416, 0.0553,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          ...,\n",
            "          [0.0561, 0.0355, 0.0217,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0561, 0.0355, 0.0217,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0561, 0.0355, 0.0217,  ..., 0.0494, 0.0494, 0.0494]],\n",
            "\n",
            "         [[0.0249, 0.0285, 0.0241,  ..., 0.0729, 0.0729, 0.0729],\n",
            "          [0.0621, 0.0946, 0.0584,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0403, 0.0592, 0.0453,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          ...,\n",
            "          [0.0722, 0.0442, 0.0496,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0722, 0.0442, 0.0496,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0722, 0.0442, 0.0496,  ..., 0.0393, 0.0393, 0.0393]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0266, 0.0353, 0.0523,  ..., 0.0295, 0.0528, 0.0929],\n",
            "          [0.0424, 0.0322, 0.0605,  ..., 0.0413, 0.0496, 0.0698],\n",
            "          [0.0485, 0.0534, 0.0663,  ..., 0.0320, 0.0630, 0.0214],\n",
            "          ...,\n",
            "          [0.0591, 0.0707, 0.0873,  ..., 0.0328, 0.0468, 0.0351],\n",
            "          [0.0850, 0.0445, 0.0542,  ..., 0.0330, 0.0824, 0.0366],\n",
            "          [0.0372, 0.0564, 0.0356,  ..., 0.0267, 0.0670, 0.0550]],\n",
            "\n",
            "         [[0.0465, 0.0293, 0.0404,  ..., 0.1094, 0.0445, 0.0386],\n",
            "          [0.0345, 0.0840, 0.0470,  ..., 0.0350, 0.0403, 0.0478],\n",
            "          [0.0332, 0.0378, 0.0375,  ..., 0.0410, 0.0408, 0.0280],\n",
            "          ...,\n",
            "          [0.0480, 0.0442, 0.0396,  ..., 0.0623, 0.0601, 0.0407],\n",
            "          [0.0597, 0.0848, 0.0398,  ..., 0.0603, 0.0403, 0.1054],\n",
            "          [0.0419, 0.0646, 0.0321,  ..., 0.0402, 0.0487, 0.0390]],\n",
            "\n",
            "         [[0.0432, 0.0414, 0.0506,  ..., 0.0729, 0.0438, 0.0714],\n",
            "          [0.0651, 0.0672, 0.0377,  ..., 0.0550, 0.0393, 0.0437],\n",
            "          [0.0522, 0.0634, 0.0232,  ..., 0.0242, 0.0556, 0.0655],\n",
            "          ...,\n",
            "          [0.0333, 0.0483, 0.0891,  ..., 0.0530, 0.0433, 0.0374],\n",
            "          [0.0364, 0.0258, 0.0630,  ..., 0.0362, 0.0403, 0.0457],\n",
            "          [0.0265, 0.0659, 0.0819,  ..., 0.0668, 0.0660, 0.0441]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0318, 0.0635, 0.0397,  ..., 0.0393, 0.0408, 0.0440],\n",
            "          [0.0283, 0.0511, 0.0654,  ..., 0.0559, 0.0587, 0.0625],\n",
            "          [0.0366, 0.0560, 0.0456,  ..., 0.0670, 0.0517, 0.0371],\n",
            "          ...,\n",
            "          [0.0653, 0.0552, 0.0442,  ..., 0.0347, 0.0687, 0.0329],\n",
            "          [0.0602, 0.0466, 0.0638,  ..., 0.0749, 0.0785, 0.0853],\n",
            "          [0.0326, 0.0651, 0.0850,  ..., 0.0550, 0.0671, 0.0963]],\n",
            "\n",
            "         [[0.0486, 0.0715, 0.0248,  ..., 0.0532, 0.0446, 0.0327],\n",
            "          [0.0551, 0.0786, 0.0511,  ..., 0.0444, 0.0483, 0.0415],\n",
            "          [0.0486, 0.0445, 0.0685,  ..., 0.0373, 0.0164, 0.0221],\n",
            "          ...,\n",
            "          [0.0450, 0.0782, 0.0659,  ..., 0.0426, 0.0663, 0.0677],\n",
            "          [0.0693, 0.0305, 0.0525,  ..., 0.0689, 0.0870, 0.0640],\n",
            "          [0.0461, 0.0653, 0.0385,  ..., 0.0444, 0.0538, 0.0567]],\n",
            "\n",
            "         [[0.0552, 0.0517, 0.0302,  ..., 0.0652, 0.0687, 0.0298],\n",
            "          [0.0688, 0.0454, 0.0366,  ..., 0.0591, 0.0286, 0.0575],\n",
            "          [0.0441, 0.0323, 0.0352,  ..., 0.0339, 0.0795, 0.0608],\n",
            "          ...,\n",
            "          [0.0967, 0.0432, 0.0428,  ..., 0.0412, 0.0213, 0.0334],\n",
            "          [0.0552, 0.0468, 0.0501,  ..., 0.0401, 0.0487, 0.0634],\n",
            "          [0.0536, 0.0383, 0.0223,  ..., 0.0457, 0.0415, 0.0443]]],\n",
            "\n",
            "\n",
            "        [[[0.0263, 0.0514, 0.0523,  ..., 0.0757, 0.0757, 0.0757],\n",
            "          [0.0477, 0.0432, 0.0722,  ..., 0.0344, 0.0344, 0.0344],\n",
            "          [0.0523, 0.0299, 0.0409,  ..., 0.0865, 0.0865, 0.0865],\n",
            "          ...,\n",
            "          [0.0793, 0.0440, 0.0795,  ..., 0.0213, 0.0213, 0.0213],\n",
            "          [0.0793, 0.0440, 0.0795,  ..., 0.0213, 0.0213, 0.0213],\n",
            "          [0.0793, 0.0440, 0.0795,  ..., 0.0213, 0.0213, 0.0213]],\n",
            "\n",
            "         [[0.0553, 0.0550, 0.0318,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          [0.0770, 0.0393, 0.0534,  ..., 0.0649, 0.0649, 0.0649],\n",
            "          [0.0700, 0.0168, 0.0782,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          ...,\n",
            "          [0.0430, 0.0266, 0.0634,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          [0.0430, 0.0266, 0.0634,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          [0.0430, 0.0266, 0.0634,  ..., 0.0525, 0.0525, 0.0525]],\n",
            "\n",
            "         [[0.0474, 0.0412, 0.0611,  ..., 0.0700, 0.0700, 0.0700],\n",
            "          [0.0346, 0.0288, 0.0437,  ..., 0.0957, 0.0957, 0.0957],\n",
            "          [0.0496, 0.0574, 0.0396,  ..., 0.0435, 0.0435, 0.0435],\n",
            "          ...,\n",
            "          [0.0413, 0.0251, 0.0665,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0413, 0.0251, 0.0665,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0413, 0.0251, 0.0665,  ..., 0.0468, 0.0468, 0.0468]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0601, 0.0245, 0.0406,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0296, 0.0392, 0.0574,  ..., 0.0630, 0.0630, 0.0630],\n",
            "          [0.0757, 0.0565, 0.0545,  ..., 0.0366, 0.0366, 0.0366],\n",
            "          ...,\n",
            "          [0.0655, 0.0317, 0.0476,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          [0.0655, 0.0317, 0.0476,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          [0.0655, 0.0317, 0.0476,  ..., 0.0555, 0.0555, 0.0555]],\n",
            "\n",
            "         [[0.0526, 0.0405, 0.0352,  ..., 0.0459, 0.0459, 0.0459],\n",
            "          [0.0704, 0.0936, 0.0404,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0280, 0.1135, 0.0316,  ..., 0.0376, 0.0376, 0.0376],\n",
            "          ...,\n",
            "          [0.0800, 0.0363, 0.0422,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          [0.0800, 0.0363, 0.0422,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          [0.0800, 0.0363, 0.0422,  ..., 0.0508, 0.0508, 0.0508]],\n",
            "\n",
            "         [[0.0403, 0.0542, 0.0622,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0364, 0.0604, 0.0479,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0283, 0.0877, 0.0483,  ..., 0.0533, 0.0533, 0.0533],\n",
            "          ...,\n",
            "          [0.0361, 0.0241, 0.0413,  ..., 0.0356, 0.0356, 0.0356],\n",
            "          [0.0361, 0.0241, 0.0413,  ..., 0.0356, 0.0356, 0.0356],\n",
            "          [0.0361, 0.0241, 0.0413,  ..., 0.0356, 0.0356, 0.0356]]],\n",
            "\n",
            "\n",
            "        [[[0.0665, 0.0279, 0.0346,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          [0.0495, 0.0424, 0.0418,  ..., 0.0337, 0.0337, 0.0337],\n",
            "          [0.0454, 0.0248, 0.0541,  ..., 0.0553, 0.0553, 0.0553],\n",
            "          ...,\n",
            "          [0.0424, 0.0529, 0.0622,  ..., 0.0256, 0.0256, 0.0256],\n",
            "          [0.0424, 0.0529, 0.0622,  ..., 0.0256, 0.0256, 0.0256],\n",
            "          [0.0424, 0.0529, 0.0622,  ..., 0.0256, 0.0256, 0.0256]],\n",
            "\n",
            "         [[0.0497, 0.0210, 0.0309,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0492, 0.0366, 0.0297,  ..., 0.0605, 0.0605, 0.0605],\n",
            "          [0.0415, 0.0428, 0.0671,  ..., 0.0607, 0.0607, 0.0607],\n",
            "          ...,\n",
            "          [0.0698, 0.0262, 0.0271,  ..., 0.0516, 0.0516, 0.0516],\n",
            "          [0.0698, 0.0262, 0.0271,  ..., 0.0516, 0.0516, 0.0516],\n",
            "          [0.0698, 0.0262, 0.0271,  ..., 0.0516, 0.0516, 0.0516]],\n",
            "\n",
            "         [[0.0799, 0.0248, 0.0278,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0608, 0.0250, 0.0504,  ..., 0.0831, 0.0831, 0.0831],\n",
            "          [0.0383, 0.0626, 0.0696,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          ...,\n",
            "          [0.0564, 0.0289, 0.0193,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0564, 0.0289, 0.0193,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0564, 0.0289, 0.0193,  ..., 0.0539, 0.0539, 0.0539]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0243, 0.0618, 0.0485,  ..., 0.0384, 0.0384, 0.0384],\n",
            "          [0.0708, 0.0358, 0.0257,  ..., 0.0576, 0.0576, 0.0576],\n",
            "          [0.0615, 0.0280, 0.0325,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          ...,\n",
            "          [0.0302, 0.0342, 0.0651,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0302, 0.0342, 0.0651,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0302, 0.0342, 0.0651,  ..., 0.0599, 0.0599, 0.0599]],\n",
            "\n",
            "         [[0.0532, 0.0438, 0.0595,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          [0.0452, 0.0982, 0.0472,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          [0.0518, 0.0625, 0.0413,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          ...,\n",
            "          [0.0394, 0.0365, 0.0394,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0394, 0.0365, 0.0394,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0394, 0.0365, 0.0394,  ..., 0.0511, 0.0511, 0.0511]],\n",
            "\n",
            "         [[0.0732, 0.0757, 0.0362,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0890, 0.0584, 0.0435,  ..., 0.0336, 0.0336, 0.0336],\n",
            "          [0.0632, 0.0689, 0.0398,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          ...,\n",
            "          [0.0384, 0.0269, 0.0873,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          [0.0384, 0.0269, 0.0873,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          [0.0384, 0.0269, 0.0873,  ..., 0.0397, 0.0397, 0.0397]]]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d3f539-fb79-4cde-abb1-6476b86cf81f"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### **각 head의 결과물 병합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear transformation합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f62395-8ff4-4f0b-b407-f484f2718361"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\r\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c645a9-2dfe-4097-cb08-85c8e5167aa4"
      },
      "source": [
        "# w_0 : (d_model, d_model)\r\n",
        "# 아무 의미 없지 않다\r\n",
        "# 서로 다른 의미로 foucsing 된 self-attention 정보들을 합쳐주는 역할 수행\r\n",
        "outputs = w_0(attn_values)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0390, -0.1240,  0.0406,  ...,  0.0479,  0.0073,  0.1110],\n",
            "         [ 0.0392, -0.1011,  0.0391,  ...,  0.0395,  0.0235,  0.0970],\n",
            "         [ 0.0521, -0.1119,  0.0252,  ...,  0.0495,  0.0509,  0.1050],\n",
            "         ...,\n",
            "         [ 0.0634, -0.1406,  0.0485,  ...,  0.0662,  0.0739,  0.0924],\n",
            "         [ 0.0634, -0.1406,  0.0485,  ...,  0.0662,  0.0739,  0.0924],\n",
            "         [ 0.0634, -0.1406,  0.0485,  ...,  0.0662,  0.0739,  0.0924]],\n",
            "\n",
            "        [[ 0.3256, -0.1864,  0.1352,  ..., -0.0625, -0.0321, -0.1640],\n",
            "         [ 0.2821, -0.1365,  0.0807,  ..., -0.0504,  0.0488, -0.1291],\n",
            "         [ 0.2804, -0.1647,  0.1189,  ..., -0.0888, -0.0025, -0.0983],\n",
            "         ...,\n",
            "         [ 0.3385, -0.1528,  0.0872,  ..., -0.0841,  0.0037, -0.1372],\n",
            "         [ 0.3385, -0.1528,  0.0872,  ..., -0.0841,  0.0037, -0.1372],\n",
            "         [ 0.3385, -0.1528,  0.0872,  ..., -0.0841,  0.0037, -0.1372]],\n",
            "\n",
            "        [[ 0.1550, -0.1998,  0.1456,  ..., -0.1398,  0.0941, -0.0777],\n",
            "         [ 0.2585, -0.1684,  0.1452,  ..., -0.1028,  0.0869, -0.0699],\n",
            "         [ 0.2452, -0.1483,  0.1038,  ..., -0.0602,  0.1504, -0.0573],\n",
            "         ...,\n",
            "         [ 0.2886, -0.1127,  0.1034,  ..., -0.1415,  0.1226, -0.0906],\n",
            "         [ 0.2886, -0.1127,  0.1034,  ..., -0.1415,  0.1226, -0.0906],\n",
            "         [ 0.2886, -0.1127,  0.1034,  ..., -0.1415,  0.1226, -0.0906]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0092,  0.0259,  0.0198,  ..., -0.1100,  0.0913,  0.0682],\n",
            "         [ 0.0675,  0.0215,  0.0539,  ..., -0.1169,  0.1389,  0.0228],\n",
            "         [ 0.0137,  0.0352,  0.0206,  ..., -0.1239,  0.0908,  0.0263],\n",
            "         ...,\n",
            "         [-0.0091,  0.0829,  0.0403,  ..., -0.1198,  0.0794,  0.0596],\n",
            "         [ 0.0161,  0.0123,  0.0114,  ..., -0.1067,  0.1343,  0.0191],\n",
            "         [-0.0348,  0.0350,  0.0020,  ..., -0.1007,  0.0844,  0.0124]],\n",
            "\n",
            "        [[-0.0621, -0.0102,  0.0328,  ..., -0.1219,  0.0871, -0.0558],\n",
            "         [ 0.0205, -0.0103,  0.0192,  ..., -0.1580,  0.1358, -0.1007],\n",
            "         [-0.0656,  0.0480, -0.0287,  ..., -0.0028,  0.0745, -0.0334],\n",
            "         ...,\n",
            "         [ 0.0069,  0.0282, -0.0207,  ..., -0.1412,  0.0921, -0.0717],\n",
            "         [ 0.0069,  0.0282, -0.0207,  ..., -0.1412,  0.0921, -0.0717],\n",
            "         [ 0.0069,  0.0282, -0.0207,  ..., -0.1412,  0.0921, -0.0717]],\n",
            "\n",
            "        [[ 0.0910, -0.0055,  0.0789,  ..., -0.0870,  0.1013, -0.0730],\n",
            "         [ 0.1359, -0.0291,  0.0756,  ..., -0.1563,  0.1764, -0.1067],\n",
            "         [ 0.0325, -0.0115,  0.0576,  ..., -0.0989,  0.1426, -0.0651],\n",
            "         ...,\n",
            "         [ 0.1625,  0.0028,  0.0252,  ..., -0.1317,  0.1442, -0.0748],\n",
            "         [ 0.1625,  0.0028,  0.0252,  ..., -0.1317,  0.1442, -0.0748],\n",
            "         [ 0.1625,  0.0028,  0.0252,  ..., -0.1317,  0.1442, -0.0748]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    # linear transformation\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    # head만큼 쪼개준다\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  # scaled-dot product attention\r\n",
        "  def self_attention(self, q, k, v):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d94e879-c642-4376-c67b-302270d8ce26"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0714, -0.0246, -0.1465,  ...,  0.1340,  0.2671, -0.0991],\n",
            "         [-0.0478, -0.0892, -0.1207,  ...,  0.1031,  0.1928, -0.0121],\n",
            "         [ 0.0668, -0.0034, -0.1259,  ...,  0.0757,  0.1786, -0.1021],\n",
            "         ...,\n",
            "         [ 0.0262,  0.0057, -0.1877,  ...,  0.1759,  0.2441, -0.0821],\n",
            "         [ 0.0262,  0.0057, -0.1877,  ...,  0.1759,  0.2441, -0.0821],\n",
            "         [ 0.0262,  0.0057, -0.1877,  ...,  0.1759,  0.2441, -0.0821]],\n",
            "\n",
            "        [[ 0.3932, -0.1357, -0.3062,  ...,  0.1884,  0.1783, -0.1680],\n",
            "         [ 0.4036, -0.1211, -0.2892,  ...,  0.2104,  0.1763, -0.1609],\n",
            "         [ 0.4562, -0.1351, -0.2547,  ...,  0.1734,  0.2246, -0.1549],\n",
            "         ...,\n",
            "         [ 0.3332, -0.1106, -0.2841,  ...,  0.1888,  0.2551, -0.1363],\n",
            "         [ 0.3332, -0.1106, -0.2841,  ...,  0.1888,  0.2551, -0.1363],\n",
            "         [ 0.3332, -0.1106, -0.2841,  ...,  0.1888,  0.2551, -0.1363]],\n",
            "\n",
            "        [[ 0.1764, -0.1598, -0.1772,  ...,  0.1703,  0.1059, -0.1156],\n",
            "         [ 0.1458, -0.1457, -0.0733,  ...,  0.0678,  0.1343, -0.1230],\n",
            "         [ 0.1654, -0.0948, -0.1240,  ...,  0.1225,  0.0563, -0.1466],\n",
            "         ...,\n",
            "         [ 0.0997, -0.0972, -0.1555,  ...,  0.0571,  0.2159, -0.0726],\n",
            "         [ 0.0997, -0.0972, -0.1555,  ...,  0.0571,  0.2159, -0.0726],\n",
            "         [ 0.0997, -0.0972, -0.1555,  ...,  0.0571,  0.2159, -0.0726]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0207,  0.0336, -0.1815,  ..., -0.0240, -0.0624, -0.0330],\n",
            "         [-0.0009,  0.0656, -0.1911,  ...,  0.0096, -0.1054, -0.0313],\n",
            "         [ 0.0108,  0.0475, -0.1605,  ..., -0.0685, -0.0733, -0.0275],\n",
            "         ...,\n",
            "         [ 0.0790,  0.0309, -0.1178,  ..., -0.0814, -0.0824,  0.0054],\n",
            "         [ 0.0189,  0.0363, -0.1564,  ..., -0.0160, -0.0631,  0.0225],\n",
            "         [ 0.0093,  0.0120, -0.1842,  ..., -0.0654, -0.0897,  0.0151]],\n",
            "\n",
            "        [[-0.0250, -0.0536, -0.2043,  ...,  0.1105,  0.0388,  0.0297],\n",
            "         [-0.0433, -0.0320, -0.1262,  ...,  0.1525,  0.1092,  0.0081],\n",
            "         [-0.0390, -0.0487, -0.1101,  ...,  0.0789,  0.0610,  0.0084],\n",
            "         ...,\n",
            "         [-0.0153, -0.0072, -0.1362,  ...,  0.0352,  0.0791,  0.0420],\n",
            "         [-0.0153, -0.0072, -0.1362,  ...,  0.0352,  0.0791,  0.0420],\n",
            "         [-0.0153, -0.0072, -0.1362,  ...,  0.0352,  0.0791,  0.0420]],\n",
            "\n",
            "        [[ 0.0343, -0.1807, -0.2029,  ...,  0.0639,  0.0377, -0.0023],\n",
            "         [ 0.0831, -0.1093, -0.1888,  ...,  0.1359,  0.0888, -0.0272],\n",
            "         [ 0.0825, -0.0846, -0.2422,  ...,  0.0770,  0.0356, -0.0199],\n",
            "         ...,\n",
            "         [ 0.1112, -0.1179, -0.2189,  ...,  0.0531,  0.0217,  0.0291],\n",
            "         [ 0.1112, -0.1179, -0.2189,  ...,  0.0531,  0.0217,  0.0291],\n",
            "         [ 0.1112, -0.1179, -0.2189,  ...,  0.0531,  0.0217,  0.0291]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}