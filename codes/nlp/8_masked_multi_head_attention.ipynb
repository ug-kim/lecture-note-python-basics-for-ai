{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_masked_multi_head_attention.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**8. Masked Multi-head Attention**\r\n",
        "1. Masked Multi-head Attention 구현.\r\n",
        "2. Encoder-Decoder Attention 구현.\r\n",
        "- Decoder 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v-8hiNHV4SG",
        "outputId": "6e9d2d89-0012-4859-8e20-557305052abe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfTSaGYteuze"
      },
      "source": [
        "데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "# 실제 값 확인해 볼 필요\r\n",
        "# 보기 쉽기 위해 데이터를 줄였다\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23]\r\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f10bbb-284f-4119-98cc-4d34abaa2d8f"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 12679.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28644a63-f63a-4a26-80f8-3b4df4e656cc"
      },
      "source": [
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],\n",
              " [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "# 보기 쉽게 하기 위해 차원도 줄였다\r\n",
        "d_model = 8  # model의 hidden size\r\n",
        "num_heads = 2  # head의 개수\r\n",
        "inf = 1e12"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7004be-1b02-4eab-8f8d-a381b16b66e5"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.9610e+00,  1.6843e-01,  2.4753e-02,  1.9252e+00, -1.1563e+00,\n",
            "          -9.6380e-01, -7.5435e-01, -1.5953e-01],\n",
            "         [-1.6101e-01, -3.4729e-01,  1.3322e-01,  2.2014e+00,  1.4778e-01,\n",
            "          -1.7889e+00, -1.1637e+00, -5.3178e-01],\n",
            "         [ 3.4615e-02,  3.4697e-01, -8.8172e-01,  7.1375e-01,  6.5656e-01,\n",
            "           2.7853e-01,  1.5836e-01, -5.0664e-01],\n",
            "         [-1.9087e-01,  7.1909e-01,  1.8414e+00,  1.5429e+00,  6.4760e-01,\n",
            "          -4.7824e-01,  9.9462e-02,  2.3132e-01],\n",
            "         [ 5.5992e-01,  2.6660e+00, -7.6602e-02,  2.3276e-01,  9.4208e-01,\n",
            "          -1.3384e+00, -1.2717e-01, -1.4878e+00],\n",
            "         [ 1.2408e+00,  5.1148e-01, -4.4437e-01,  2.1889e-02, -5.8662e-01,\n",
            "           3.8692e-01, -6.3511e-01, -1.9990e+00],\n",
            "         [-5.9062e-01,  3.6413e-01, -3.6650e-01, -5.3939e-02, -1.4821e+00,\n",
            "          -1.0151e+00, -1.8241e+00, -7.1189e-01],\n",
            "         [-1.6101e-01, -3.4729e-01,  1.3322e-01,  2.2014e+00,  1.4778e-01,\n",
            "          -1.7889e+00, -1.1637e+00, -5.3178e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01]],\n",
            "\n",
            "        [[ 2.6301e-01,  8.6068e-01, -2.9850e-01,  1.5376e+00,  3.1190e-01,\n",
            "           2.7547e-01, -1.1019e+00,  3.5926e-01],\n",
            "         [-8.8410e-02, -5.8146e-01,  2.2859e-01,  9.4652e-01,  9.6267e-01,\n",
            "           3.4401e-01,  1.1082e+00,  8.2263e-01],\n",
            "         [-2.4021e-02,  1.0448e+00,  2.9782e-02,  5.4397e-01, -1.7108e-01,\n",
            "           9.9046e-02, -1.1697e+00,  1.2611e+00],\n",
            "         [ 9.9465e-01, -4.7529e-02, -6.6690e-01, -5.9739e-01,  1.7982e+00,\n",
            "           3.4953e-01, -4.6744e-02, -2.1303e+00],\n",
            "         [ 1.0270e+00,  9.8412e-01,  3.8174e-01,  1.1592e+00, -8.5306e-01,\n",
            "          -5.3041e-01, -3.8399e-01, -3.0884e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01]],\n",
            "\n",
            "        [[-2.8427e-01,  1.2050e+00,  4.6022e-03, -1.0238e+00,  1.6124e+00,\n",
            "          -5.3490e-01,  3.4694e-01,  1.1622e+00],\n",
            "         [-1.3695e+00,  7.3960e-01,  1.0495e+00,  2.6287e-02,  1.0779e+00,\n",
            "          -1.0952e+00, -1.2651e+00,  7.1061e-01],\n",
            "         [-1.0649e+00, -6.9567e-01, -1.3952e+00,  4.6535e-01, -8.3085e-02,\n",
            "          -4.5886e-01,  4.4427e-01,  7.3219e-01],\n",
            "         [-1.1076e+00, -8.0857e-01, -1.7190e+00, -2.9061e-01, -2.1303e+00,\n",
            "          -1.9790e+00, -3.5036e-01,  9.0276e-01],\n",
            "         [ 4.9527e-01,  7.8284e-01, -1.2975e+00, -7.0080e-01,  4.3236e-01,\n",
            "           1.7904e-01,  2.3694e+00,  5.5529e-01],\n",
            "         [ 7.4726e-01, -1.7614e+00, -5.0371e-01, -1.8206e+00,  5.4614e-02,\n",
            "          -1.8223e+00, -8.7048e-01,  4.9222e-01],\n",
            "         [ 7.7195e-01,  2.2507e-02,  7.1324e-01,  3.3778e-01,  5.6660e-01,\n",
            "           5.3026e-01, -3.4422e-01, -8.4053e-01],\n",
            "         [ 2.3320e-01,  2.9105e-01,  1.5214e+00,  5.3186e-01,  9.0200e-01,\n",
            "           1.3446e+00,  4.5404e-02,  1.2119e+00],\n",
            "         [ 1.2979e+00, -6.4501e-01, -4.9963e-01,  8.9394e-01,  1.9550e+00,\n",
            "          -1.3361e+00, -9.1141e-02, -5.8169e-01],\n",
            "         [ 1.8863e+00,  9.7950e-01, -1.5591e+00, -1.2775e-02,  3.2478e-01,\n",
            "           4.6820e-01,  4.5149e-01,  1.0556e+00]],\n",
            "\n",
            "        [[ 4.7837e-01,  6.4858e-01,  1.1006e+00,  4.0138e-01, -1.2286e+00,\n",
            "           3.8888e-01,  2.2042e-01, -2.6676e+00],\n",
            "         [ 4.2221e-02,  2.0407e-01, -4.4206e-01, -5.2619e-01,  3.1357e-01,\n",
            "          -1.9160e+00, -9.6526e-01, -1.2759e-01],\n",
            "         [ 2.2762e+00, -7.1812e-02,  4.3952e-01,  3.9997e-01,  7.5124e-01,\n",
            "          -8.6826e-01,  1.9945e+00, -7.5891e-02],\n",
            "         [ 3.4615e-02,  3.4697e-01, -8.8172e-01,  7.1375e-01,  6.5656e-01,\n",
            "           2.7853e-01,  1.5836e-01, -5.0664e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01]],\n",
            "\n",
            "        [[-1.7074e-03,  3.5760e-01,  6.3164e-01,  1.8163e+00, -7.7992e-01,\n",
            "          -5.3810e-01,  6.0806e-01, -2.3366e-01],\n",
            "         [-1.1076e+00, -8.0857e-01, -1.7190e+00, -2.9061e-01, -2.1303e+00,\n",
            "          -1.9790e+00, -3.5036e-01,  9.0276e-01],\n",
            "         [-2.4021e-02,  1.0448e+00,  2.9782e-02,  5.4397e-01, -1.7108e-01,\n",
            "           9.9046e-02, -1.1697e+00,  1.2611e+00],\n",
            "         [-1.7074e-03,  3.5760e-01,  6.3164e-01,  1.8163e+00, -7.7992e-01,\n",
            "          -5.3810e-01,  6.0806e-01, -2.3366e-01],\n",
            "         [ 8.7901e-01, -1.8784e+00, -1.0269e+00, -1.4309e-01, -3.5501e-01,\n",
            "          -9.7908e-01, -5.2272e-02,  2.4137e+00],\n",
            "         [-1.2113e+00,  1.8682e+00,  5.5422e-01,  1.6577e+00,  1.0512e+00,\n",
            "          -8.2051e-01,  1.3147e+00, -1.0341e+00],\n",
            "         [-2.8427e-01,  1.2050e+00,  4.6022e-03, -1.0238e+00,  1.6124e+00,\n",
            "          -5.3490e-01,  3.4694e-01,  1.1622e+00],\n",
            "         [ 8.7901e-01, -1.8784e+00, -1.0269e+00, -1.4309e-01, -3.5501e-01,\n",
            "          -9.7908e-01, -5.2272e-02,  2.4137e+00],\n",
            "         [ 1.3964e-01, -9.0049e-01, -3.3447e-01,  3.9081e-01,  1.3104e+00,\n",
            "          -2.5764e-01,  3.7236e-02,  1.2437e+00],\n",
            "         [ 2.3676e-01, -2.5670e-01,  3.9976e-01, -3.7022e-01, -3.7911e-03,\n",
            "          -9.7567e-01,  5.2542e-01, -6.0942e-01]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([5, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3gxeyhpyF2"
      },
      "source": [
        "### **Mask 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDEQF64p5pN"
      },
      "source": [
        "`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0A4elupM2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cb7a49-6dab-4069-ba29-8ac217e75fa2"
      },
      "source": [
        "# pad token을 masking 해준다\r\n",
        "# True -> attention 허가, False -> masking\r\n",
        "padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)\r\n",
        "\r\n",
        "print(padding_mask)\r\n",
        "print(padding_mask.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cD54evrEo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca8ed34-dd86-4961-8250-fad786a8872e"
      },
      "source": [
        "# nopeak_mask - 보지 못하게 막는 마스크\r\n",
        "# 전부 1로 초기화\r\n",
        "# 맨 앞의 1은 계산의 용이성을 위해\r\n",
        "nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)\r\n",
        "# tril - triangle low\r\n",
        "# matrix 반쪽을 아래쪽만 True로 채워진 삼각형 모양을 만들어준다\r\n",
        "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)\r\n",
        "\r\n",
        "print(nopeak_mask)\r\n",
        "print(nopeak_mask.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "torch.Size([1, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMzB8_jarycy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f325804b-5f10-46da-8a55-ccdbdc85c49d"
      },
      "source": [
        "# padding_mask - (B, 1, L)\r\n",
        "# nopeak_mask가 batch_size만큼 복사되고, \r\n",
        "# padding mask가 또 L만큼 반복되어\r\n",
        "# 둘이 같은 사이즈로 맞춰지면서\r\n",
        "# 동시에 둘이 함께 True였던 부분만 True가 되고 나머지는 False가 된다\r\n",
        "mask = padding_mask & nopeak_mask  # (B, L, L)\r\n",
        "\r\n",
        "print(mask)\r\n",
        "print(mask.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985918e3-5e0b-47b1-d3a9-3bc750fb6d84"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Masking이 적용된 self-attention 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqaQmVQdvMZB"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adlRCt6mvMy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a473631-1996-4b7c-ab18-0016fefb2ee4"
      },
      "source": [
        "masks = mask.unsqueeze(1)  # (B, 1, L, L)\r\n",
        "# masked_fill_ 함수 -> mask중 False는 -inf으로\r\n",
        "# 0이 아니고 매우 작은 값을 넣어줬다\r\n",
        "masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(masked_attn_scores)\r\n",
        "print(masked_attn_scores.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 5.9993e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.2600e-01,  4.3161e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.9480e-01,  5.1267e-01,  1.6712e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 9.6691e-01,  1.1695e+00,  2.2937e-01,  8.9673e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.5012e-01,  4.4531e-01,  2.0075e-01,  1.4426e-01,  1.1765e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 7.1670e-02,  7.0278e-02,  8.7590e-04, -3.7849e-02,  3.2966e-02,\n",
            "           -8.0640e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.0063e-01, -3.8789e-01, -1.6093e-01, -5.8082e-01, -7.1848e-02,\n",
            "           -2.2517e-01, -4.9220e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.2600e-01,  4.3161e-01,  4.1901e-02, -4.3872e-02,  1.2474e-01,\n",
            "           -3.1987e-01,  3.0502e-01,  4.3161e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.6230e-01, -3.3520e-01, -1.3036e-01, -3.6478e-01, -1.9587e-01,\n",
            "           -4.7273e-02, -6.2809e-02, -3.3520e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.6230e-01, -3.3520e-01, -1.3036e-01, -3.6478e-01, -1.9587e-01,\n",
            "           -4.7273e-02, -6.2809e-02, -3.3520e-01, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[ 3.4329e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.2894e-01, -1.2385e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.7359e-02,  2.6770e-02,  7.5827e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.3565e-01,  2.7016e-01,  2.6853e-02, -2.4620e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.5593e-01,  6.4159e-01,  3.5910e-01,  5.2779e-02, -2.0583e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.4278e-01, -3.8555e-01,  3.0138e-01,  1.6286e-01,  8.2358e-01,\n",
            "            1.2875e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.9716e-01,  4.1791e-02,  1.4324e-01,  9.9641e-02,  2.5490e-01,\n",
            "           -3.8517e-01, -4.6730e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.2894e-01, -1.2385e-02,  1.5912e-01, -2.2595e-02, -3.3695e-01,\n",
            "           -2.5248e-01, -1.8838e-01, -1.2385e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4588e-02,  2.6232e-01,  8.3552e-02,  4.7705e-03, -1.5560e-01,\n",
            "           -3.2868e-02, -5.4777e-02,  2.6232e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4588e-02,  2.6232e-01,  8.3552e-02,  4.7705e-03, -1.5560e-01,\n",
            "           -3.2868e-02, -5.4777e-02,  2.6232e-01, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 4.3693e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.7605e-01,  2.4136e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.4436e-01,  2.7885e-02,  9.2276e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.4888e-01,  1.8120e-01,  2.9541e-01, -1.0319e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.5781e-02,  2.2456e-03, -2.8826e-02, -1.6147e-03, -1.3537e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.3776e-01, -1.4706e-01, -2.3922e-01, -1.7757e-02, -2.1237e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.3776e-01, -1.4706e-01, -2.3922e-01, -1.7757e-02, -2.1237e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.3776e-01, -1.4706e-01, -2.3922e-01, -1.7757e-02, -2.1237e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.3776e-01, -1.4706e-01, -2.3922e-01, -1.7757e-02, -2.1237e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.3776e-01, -1.4706e-01, -2.3922e-01, -1.7757e-02, -2.1237e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[ 3.4621e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4642e-02, -1.1144e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.9588e-03, -3.3112e-02, -6.8410e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4664e-01,  2.6480e-01,  1.2722e-01,  3.6701e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 9.8376e-02,  1.4871e-01,  1.2343e-02,  3.2317e-01, -7.2031e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9683e-01,  1.0236e-01,  5.8380e-02,  7.0997e-02, -1.1427e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9683e-01,  1.0236e-01,  5.8380e-02,  7.0997e-02, -1.1427e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9683e-01,  1.0236e-01,  5.8380e-02,  7.0997e-02, -1.1427e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9683e-01,  1.0236e-01,  5.8380e-02,  7.0997e-02, -1.1427e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9683e-01,  1.0236e-01,  5.8380e-02,  7.0997e-02, -1.1427e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3577e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0093e-01,  6.2396e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.6389e-01, -9.0058e-02,  4.1881e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.5034e-02, -9.4550e-01,  1.4723e-01,  5.5997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.9695e-02, -4.4359e-01,  3.0083e-01,  4.1476e-01,  4.8111e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.4068e-01, -1.0505e+00, -6.6122e-01, -6.1433e-01,  2.0357e-01,\n",
            "            3.6494e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.0686e-01,  5.0567e-01,  2.8078e-02,  1.2948e-02, -2.9784e-01,\n",
            "           -6.8498e-02,  2.1565e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.6164e-01,  8.8684e-01, -1.9382e-02, -1.7249e-01, -5.6672e-01,\n",
            "           -2.9087e-01,  4.6013e-01,  7.9290e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [ 8.6523e-02,  8.1593e-02,  1.9608e-01,  3.1739e-01,  8.4377e-02,\n",
            "            2.1448e-01, -1.0710e-01, -1.4203e-01,  8.2270e-02, -1.0000e+12],\n",
            "          [ 2.9949e-02, -1.9065e-01,  9.4942e-02,  2.2021e-02,  1.9673e-01,\n",
            "            4.6962e-02, -1.7608e-01, -3.2533e-01, -8.6870e-02, -1.0947e-01]],\n",
            "\n",
            "         [[-6.5591e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.9410e-01,  2.3719e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.6777e-01,  6.4179e-02,  8.7875e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.9931e-01,  3.5082e-01,  2.5163e-02, -3.5985e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.2157e-01, -2.8920e-01,  1.6579e-01,  6.0577e-01, -1.5777e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.3465e-01,  2.3178e-01,  5.5229e-02, -2.9759e-02, -7.3775e-01,\n",
            "            2.7216e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.2455e-01,  1.5880e-02, -5.9069e-02, -2.0117e-01,  3.5769e-01,\n",
            "            1.4520e-01,  1.1597e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.0624e-02,  5.0719e-02, -5.7932e-02,  3.3436e-01, -5.5111e-02,\n",
            "            6.5573e-01, -1.1094e-03, -1.7778e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.4051e-01, -2.5039e-01,  2.2105e-01,  1.5376e-01, -2.4482e-01,\n",
            "           -1.1620e-01,  6.6025e-02,  1.1077e-01,  7.5222e-02, -1.0000e+12],\n",
            "          [ 2.7004e-03, -1.5104e-01,  3.3017e-02,  2.5493e-01,  1.9108e-01,\n",
            "            4.5061e-01,  1.0866e-01,  1.7331e-03,  2.8998e-01,  3.6261e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.0113e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.8984e-01, -4.1287e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.7104e-02, -1.1165e-01,  1.1919e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.8849e-01,  3.7211e-01, -2.7892e-03,  1.6712e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.3177e-02, -2.1353e-02, -1.4850e-02, -1.3036e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[ 7.9663e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.8399e-01,  2.1701e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4419e-01, -3.0110e-02,  4.4764e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.6405e-03, -7.6756e-02,  1.8064e-03,  7.5827e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4535e-01,  1.2419e-02, -1.8020e-02,  8.3552e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5768e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.4292e+00,  5.5997e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.4178e-02,  4.1870e-02,  9.2276e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.5768e-01,  8.7899e-01,  1.6487e-01,  1.5768e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0250e+00, -1.5997e-01, -9.9823e-01, -1.0250e+00, -5.1597e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 7.5130e-01,  1.2655e+00,  7.3267e-01,  7.5130e-01,  1.1389e+00,\n",
            "            6.9138e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.1782e-02, -1.4957e-01,  6.4054e-02,  5.1782e-02, -1.1918e-01,\n",
            "            1.4888e-01,  2.3577e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0250e+00, -1.5997e-01, -9.9823e-01, -1.0250e+00, -5.1597e-01,\n",
            "           -7.6929e-01, -1.5989e-01, -5.1597e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0367e-02,  9.4172e-02,  1.4890e-02, -1.0367e-02,  1.5086e-02,\n",
            "            8.4652e-02,  7.8903e-02,  1.5086e-02,  4.8493e-02, -1.0000e+12],\n",
            "          [-2.2338e-01,  1.8315e-01, -2.3922e-01, -2.2338e-01,  8.6536e-02,\n",
            "           -2.9619e-01, -5.0556e-02,  8.6536e-02, -1.1696e-01, -1.0000e+12]],\n",
            "\n",
            "         [[-1.1586e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.2080e-01, -3.5985e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.8542e-01, -9.9180e-02, -6.8410e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1586e-02, -2.5066e-01,  2.2820e-02, -1.1586e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.4772e-02,  3.0579e-01, -7.5729e-02, -6.4772e-02, -8.8913e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.5678e-01,  1.5309e-01,  2.1374e-01,  1.5678e-01,  4.9275e-01,\n",
            "           -4.3090e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.6055e-02,  6.3498e-01,  1.1582e-01,  5.6055e-02,  1.3279e+00,\n",
            "           -1.0268e+00, -6.5591e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.4772e-02,  3.0579e-01, -7.5729e-02, -6.4772e-02, -8.8913e-02,\n",
            "           -2.6228e-01, -4.2743e-01, -8.8913e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.1806e-02,  4.3668e-01,  2.3040e-02,  5.1806e-02,  3.4476e-01,\n",
            "           -4.7079e-01, -5.1325e-01,  3.4476e-01,  1.7806e-02, -1.0000e+12],\n",
            "          [-2.5899e-02, -6.0330e-02,  5.8380e-02, -2.5899e-02,  2.3334e-01,\n",
            "           -1.5227e-01, -1.0696e-01,  2.3334e-01,  2.2539e-01, -1.0000e+12]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "torch.Size([5, 2, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EqMuVJFwHhI"
      },
      "source": [
        "`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVNze4elv4Uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c353413e-497e-410c-d6f5-d621a5cb7220"
      },
      "source": [
        "attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4986, 0.5014, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3651, 0.3717, 0.2631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2751, 0.3369, 0.1316, 0.2564, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2365, 0.2354, 0.1843, 0.1742, 0.1696, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1771, 0.1768, 0.1650, 0.1587, 0.1704, 0.1521, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1343, 0.1231, 0.1545, 0.1015, 0.1689, 0.1449, 0.1728, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1558, 0.1567, 0.1061, 0.0974, 0.1153, 0.0739, 0.1381, 0.1567,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1186, 0.1103, 0.1353, 0.1070, 0.1267, 0.1470, 0.1448, 0.1103,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1186, 0.1103, 0.1353, 0.1070, 0.1267, 0.1470, 0.1448, 0.1103,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5845, 0.4155, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3280, 0.3278, 0.3443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2569, 0.2939, 0.2304, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1413, 0.3137, 0.2365, 0.1741, 0.1344, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0735, 0.0951, 0.1890, 0.1646, 0.3187, 0.1591, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1222, 0.1552, 0.1717, 0.1644, 0.1920, 0.1012, 0.0933, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1774, 0.1261, 0.1497, 0.1248, 0.0911, 0.0992, 0.1057, 0.1261,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1208, 0.1532, 0.1281, 0.1184, 0.1008, 0.1140, 0.1115, 0.1532,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1208, 0.1532, 0.1281, 0.1184, 0.1008, 0.1140, 0.1115, 0.1532,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5336, 0.4664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3522, 0.3135, 0.3343, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2916, 0.2465, 0.2764, 0.1855, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1958, 0.2096, 0.2032, 0.2088, 0.1826, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1717, 0.2078, 0.1895, 0.2364, 0.1946, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1717, 0.2078, 0.1895, 0.2364, 0.1946, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1717, 0.2078, 0.1895, 0.2364, 0.1946, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1717, 0.2078, 0.1895, 0.2364, 0.1946, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1717, 0.2078, 0.1895, 0.2364, 0.1946, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5217, 0.4783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3453, 0.3331, 0.3216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2479, 0.2525, 0.2200, 0.2796, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1974, 0.2076, 0.1812, 0.2472, 0.1665, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2235, 0.2034, 0.1946, 0.1971, 0.1815, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2235, 0.2034, 0.1946, 0.1971, 0.1815, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2235, 0.2034, 0.1946, 0.1971, 0.1815, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2235, 0.2034, 0.1946, 0.1971, 0.1815, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2235, 0.2034, 0.1946, 0.1971, 0.1815, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3958, 0.6042, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3261, 0.2530, 0.4208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2300, 0.0907, 0.2705, 0.4088, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1593, 0.1053, 0.2216, 0.2484, 0.2654, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1623, 0.0798, 0.1178, 0.1235, 0.2798, 0.2367, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1440, 0.2145, 0.1331, 0.1311, 0.0961, 0.1208, 0.1605, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1116, 0.2304, 0.0931, 0.0799, 0.0539, 0.0710, 0.1504, 0.2098,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1096, 0.1091, 0.1223, 0.1381, 0.1094, 0.1246, 0.0903, 0.0872,\n",
            "           0.1092, 0.0000],\n",
            "          [0.1071, 0.0859, 0.1143, 0.1063, 0.1266, 0.1090, 0.0872, 0.0751,\n",
            "           0.0953, 0.0932]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3249, 0.6751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2250, 0.3829, 0.3921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1908, 0.3656, 0.2640, 0.1796, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1244, 0.1420, 0.2239, 0.3476, 0.1620, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1347, 0.2148, 0.1800, 0.1654, 0.0815, 0.2236, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1758, 0.1291, 0.1198, 0.1039, 0.1817, 0.1469, 0.1427, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1129, 0.1152, 0.1033, 0.1530, 0.1036, 0.2109, 0.1094, 0.0917,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0734, 0.0887, 0.1422, 0.1329, 0.0892, 0.1015, 0.1218, 0.1273,\n",
            "           0.1229, 0.0000],\n",
            "          [0.0846, 0.0725, 0.0872, 0.1088, 0.1021, 0.1324, 0.0940, 0.0845,\n",
            "           0.1127, 0.1212]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4629, 0.5371, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3229, 0.2997, 0.3775, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1711, 0.3313, 0.2277, 0.2699, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2446, 0.2602, 0.2619, 0.2333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3306, 0.6694, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3349, 0.2546, 0.4105, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2480, 0.2316, 0.2506, 0.2698, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2025, 0.2620, 0.2542, 0.2813, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1203, 0.8797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3216, 0.3306, 0.3477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1975, 0.4062, 0.1989, 0.1975, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1415, 0.3361, 0.1454, 0.1415, 0.2355, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1415, 0.2366, 0.1388, 0.1415, 0.2084, 0.1332, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1482, 0.1212, 0.1500, 0.1482, 0.1249, 0.1633, 0.1441, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0807, 0.1916, 0.0829, 0.0807, 0.1342, 0.1042, 0.1916, 0.1342,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1059, 0.1176, 0.1086, 0.1059, 0.1087, 0.1165, 0.1158, 0.1087,\n",
            "           0.1123, 0.0000],\n",
            "          [0.0958, 0.1438, 0.0943, 0.0958, 0.1305, 0.0890, 0.1138, 0.1305,\n",
            "           0.1065, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5098, 0.4902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3111, 0.3391, 0.3497, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2616, 0.2060, 0.2708, 0.2616, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1847, 0.2676, 0.1827, 0.1847, 0.1803, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1664, 0.1658, 0.1761, 0.1664, 0.2328, 0.0925, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1082, 0.1930, 0.1149, 0.1082, 0.3860, 0.0366, 0.0531, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1265, 0.1832, 0.1251, 0.1265, 0.1235, 0.1038, 0.0880, 0.1235,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1081, 0.1589, 0.1050, 0.1081, 0.1449, 0.0641, 0.0614, 0.1449,\n",
            "           0.1045, 0.0000],\n",
            "          [0.1027, 0.0992, 0.1118, 0.1027, 0.1331, 0.0905, 0.0947, 0.1331,\n",
            "           0.1321, 0.0000]]]], grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([5, 2, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwm34bswV7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8663cd16-33cb-4d89-e91e-ec8bad4c792b"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Xab7WKzTEU"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlF7R6DIzVWc"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v, mask=None):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  def self_attention(self, q, k, v, mask=None):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    if mask is not None:\r\n",
        "      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)\r\n",
        "      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)\r\n",
        "\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5bca62d-6173-4d99-ac1d-769d37e3cfcf"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-7.0922e-03, -1.8101e-01, -8.1648e-01, -1.8661e-01, -4.4591e-01,\n",
            "          -5.5058e-01,  2.6041e-01,  6.1075e-01],\n",
            "         [-2.3712e-01, -2.4034e-01, -6.8836e-01, -4.0657e-01, -4.8102e-01,\n",
            "          -5.0028e-01,  2.3226e-01,  4.4302e-01],\n",
            "         [-3.8950e-01, -1.6009e-01, -6.1487e-01, -3.0829e-01, -1.9821e-01,\n",
            "          -4.1462e-01,  1.4663e-01,  2.9856e-01],\n",
            "         [-4.3241e-01, -9.4246e-02, -5.8908e-01, -2.7046e-01, -2.5450e-01,\n",
            "          -4.1177e-01,  2.5301e-01,  3.0859e-01],\n",
            "         [-5.3860e-01, -7.5092e-02, -5.6061e-01, -3.2771e-01, -2.1212e-01,\n",
            "          -3.7784e-01,  2.9600e-01,  3.3393e-01],\n",
            "         [-5.0542e-01, -6.8979e-03, -4.8650e-01, -2.3255e-01, -1.9498e-01,\n",
            "          -3.9364e-01,  2.9099e-01,  4.8673e-01],\n",
            "         [-4.4824e-01, -9.3124e-03, -4.8479e-01, -2.1237e-01, -2.4692e-01,\n",
            "          -4.2991e-01,  2.8655e-01,  5.7619e-01],\n",
            "         [-4.4588e-01, -6.3881e-02, -4.7409e-01, -2.6464e-01, -2.3907e-01,\n",
            "          -4.2752e-01,  2.4946e-01,  5.2912e-01],\n",
            "         [-4.5504e-01, -5.1410e-02, -4.9159e-01, -2.7082e-01, -2.8941e-01,\n",
            "          -4.3849e-01,  2.7795e-01,  5.4373e-01],\n",
            "         [-4.5504e-01, -5.1410e-02, -4.9159e-01, -2.7082e-01, -2.8941e-01,\n",
            "          -4.3849e-01,  2.7795e-01,  5.4373e-01]],\n",
            "\n",
            "        [[-7.9501e-01, -2.2850e-01, -6.2240e-01, -4.8188e-01, -1.9828e-01,\n",
            "          -3.5922e-01,  2.2485e-01, -6.0629e-02],\n",
            "         [-6.0463e-01, -1.7768e-01, -5.8562e-01, -1.7565e-01,  7.3143e-02,\n",
            "          -3.7200e-01,  6.4587e-02, -1.6362e-01],\n",
            "         [-5.9262e-01, -2.2423e-01, -5.6373e-01, -2.2371e-01, -1.2222e-02,\n",
            "          -4.1108e-01,  9.9034e-02, -7.7873e-02],\n",
            "         [-6.2914e-01, -6.3001e-02, -4.2879e-01, -1.4789e-01,  6.5057e-02,\n",
            "          -3.5872e-01,  3.4544e-02,  8.2623e-02],\n",
            "         [-6.0121e-01, -8.0915e-02, -4.1600e-01, -1.6368e-01,  1.1093e-02,\n",
            "          -3.8015e-01,  1.0398e-01,  1.6393e-01],\n",
            "         [-5.6966e-01, -9.6823e-02, -4.5155e-01, -1.5202e-01, -1.9860e-02,\n",
            "          -4.0842e-01,  1.2577e-01,  1.9549e-01],\n",
            "         [-5.6966e-01, -9.6823e-02, -4.5155e-01, -1.5202e-01, -1.9860e-02,\n",
            "          -4.0842e-01,  1.2577e-01,  1.9549e-01],\n",
            "         [-5.6966e-01, -9.6823e-02, -4.5155e-01, -1.5202e-01, -1.9860e-02,\n",
            "          -4.0842e-01,  1.2577e-01,  1.9549e-01],\n",
            "         [-5.6966e-01, -9.6823e-02, -4.5155e-01, -1.5202e-01, -1.9860e-02,\n",
            "          -4.0842e-01,  1.2577e-01,  1.9549e-01],\n",
            "         [-5.6966e-01, -9.6823e-02, -4.5155e-01, -1.5202e-01, -1.9860e-02,\n",
            "          -4.0842e-01,  1.2577e-01,  1.9549e-01]],\n",
            "\n",
            "        [[-8.3212e-01, -2.0553e-01, -4.5462e-01, -2.2428e-01,  3.1174e-01,\n",
            "          -3.9942e-01,  1.6340e-02, -7.8876e-02],\n",
            "         [-7.2840e-01, -1.4544e-01, -4.1563e-01, -3.4301e-01, -6.2420e-02,\n",
            "          -3.9233e-01,  1.3695e-01,  8.6935e-02],\n",
            "         [-4.6501e-01, -2.9216e-01, -6.0477e-01, -1.9304e-01,  1.0710e-01,\n",
            "          -4.9634e-01, -1.3052e-01,  2.5871e-02],\n",
            "         [-2.3247e-01, -3.5283e-01, -6.4082e-01, -1.1185e-01,  8.5568e-02,\n",
            "          -6.0172e-01, -2.5002e-01,  2.1247e-01],\n",
            "         [-2.4277e-01, -3.3442e-01, -6.3365e-01,  2.0876e-02,  3.0831e-01,\n",
            "          -5.9016e-01, -2.4534e-01,  2.0626e-01],\n",
            "         [-2.5081e-01, -3.4725e-01, -5.4766e-01, -2.8369e-02,  2.3196e-01,\n",
            "          -5.8456e-01, -2.1030e-01,  2.3934e-01],\n",
            "         [-2.2283e-01, -2.8257e-01, -3.8719e-01, -3.2723e-02,  1.2844e-01,\n",
            "          -5.4750e-01, -1.7392e-01,  3.1071e-01],\n",
            "         [-2.5158e-01, -2.3091e-01, -3.4891e-01,  1.5107e-03,  1.2419e-01,\n",
            "          -5.1512e-01, -1.3748e-01,  2.6294e-01],\n",
            "         [-2.9101e-01, -2.9780e-01, -4.1977e-01, -4.7844e-02,  1.8182e-01,\n",
            "          -5.1722e-01, -1.4713e-01,  1.9704e-01],\n",
            "         [-2.9939e-01, -3.0761e-01, -4.0065e-01, -2.9704e-02,  2.3984e-01,\n",
            "          -5.1851e-01, -1.8794e-01,  1.7789e-01]],\n",
            "\n",
            "        [[ 1.0961e-01,  7.2415e-01, -2.4774e-02,  7.1547e-01, -1.6435e-01,\n",
            "          -4.1546e-01,  5.2797e-01,  1.6779e+00],\n",
            "         [-1.9057e-01,  1.2225e-01, -1.7434e-01,  9.5575e-02, -1.5151e-01,\n",
            "          -5.0212e-01,  1.8767e-01,  1.0109e+00],\n",
            "         [-2.6054e-01,  1.2834e-01, -1.5806e-01,  1.9375e-01,  5.3428e-02,\n",
            "          -4.8112e-01,  1.5293e-01,  8.4924e-01],\n",
            "         [-3.9377e-01,  2.1187e-02, -2.9327e-01,  4.7293e-02,  1.0972e-01,\n",
            "          -4.4712e-01,  8.9590e-02,  5.8264e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01],\n",
            "         [-3.8352e-01,  2.0919e-02, -3.0648e-01,  8.2518e-02,  1.0915e-01,\n",
            "          -4.6428e-01,  1.1288e-01,  5.6795e-01]],\n",
            "\n",
            "        [[-1.9630e-01, -1.3718e-02, -5.8447e-01,  9.0230e-02, -8.1688e-02,\n",
            "          -4.8639e-01,  3.4164e-01,  5.3317e-01],\n",
            "         [ 1.4186e-02, -2.3177e-01, -6.3421e-01,  8.5166e-02, -3.4453e-02,\n",
            "          -6.2709e-01,  6.4012e-02,  5.7075e-01],\n",
            "         [ 8.6678e-03, -4.8840e-01, -5.9659e-01, -8.7063e-02, -8.3901e-02,\n",
            "          -7.0382e-01, -9.6490e-02,  4.6141e-01],\n",
            "         [-1.1018e-01, -2.7825e-01, -6.0341e-01, -2.0583e-02, -7.7919e-02,\n",
            "          -6.0454e-01,  8.2037e-02,  4.5925e-01],\n",
            "         [ 1.0577e-02, -3.9041e-01, -5.0519e-01,  1.0616e-03, -2.1090e-02,\n",
            "          -6.5341e-01, -6.3549e-02,  4.6148e-01],\n",
            "         [-1.9298e-01, -3.8464e-01, -6.5423e-01, -9.5523e-02, -1.1111e-02,\n",
            "          -6.1086e-01, -5.6298e-02,  2.0402e-01],\n",
            "         [-2.7240e-01, -3.3036e-01, -6.0527e-01, -9.8453e-02,  4.7423e-02,\n",
            "          -5.6743e-01, -4.0097e-02,  2.0206e-01],\n",
            "         [-3.1158e-01, -1.9076e-01, -5.4785e-01, -6.1580e-02,  8.0997e-02,\n",
            "          -4.9504e-01,  9.1554e-02,  3.3378e-01],\n",
            "         [-2.7584e-01, -3.4543e-01, -5.1632e-01, -9.8668e-02,  1.0391e-01,\n",
            "          -5.5280e-01, -9.8378e-02,  1.7552e-01],\n",
            "         [-3.6812e-01, -2.8594e-01, -6.7838e-01, -1.2636e-01,  2.2358e-02,\n",
            "          -5.2645e-01,  3.5409e-02,  9.5274e-02]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([5, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1g99JEEFwFv"
      },
      "source": [
        "### **Encoder-Decoder attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2PRoF4fF4ah"
      },
      "source": [
        "Query, key, value만 달라질 뿐 구현은 동일합니다.  \r\n",
        "Decoder에 들어갈 batch만 별도 구현하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Kcm9O81S4-"
      },
      "source": [
        "- seq2seq with attention에서 decoder hidden state와 전체 endocer hidden states들을 곱해주는\r\n",
        "- Encoder 결과와 연관성을 찾는 기준을 Transformer 기준으로 구현한 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26ra2BsGEdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee588822-5e54-43ab-8ec4-2c2208c094bf"
      },
      "source": [
        "# Decoder에 들어갈 target data\r\n",
        "trg_data = [\r\n",
        "  [33, 11, 49, 10],\r\n",
        "  [88, 34, 5, 29, 99, 45, 11, 25],\r\n",
        "  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],\r\n",
        "  [16, 58, 91, 47, 12, 5, 8],\r\n",
        "  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]\r\n",
        "]\r\n",
        "\r\n",
        "trg_data, trg_max_len = padding(trg_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 24328.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYysB4EKHKGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927c2762-01af-4fbc-e593-ee07353b73ce"
      },
      "source": [
        "# S_L: source maximum sequence length, T_L: target maximum sequence length\r\n",
        "src_batch = batch  # (B, S_L)\r\n",
        "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\r\n",
        "\r\n",
        "print(src_batch.shape)\r\n",
        "print(trg_batch.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n",
            "torch.Size([5, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieDxWYIHXKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25c0d10-4f8d-4663-fa9b-4080beefa656"
      },
      "source": [
        "src_emb = embedding(src_batch)  # (B, S_L, d_w)\r\n",
        "trg_emb = embedding(trg_batch)  # (B, T_L, d_w)\r\n",
        "\r\n",
        "print(src_emb.shape)  # encoder에서 나온 결과\r\n",
        "print(trg_emb.shape)  # masked multi-attention 하고 나온 결과라고 가정"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10, 8])\n",
            "torch.Size([5, 12, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCjmPurH2b7"
      },
      "source": [
        "`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUhY-z8JHeUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5b02a8-b10a-4514-a306-824e2246a5ed"
      },
      "source": [
        "# Query는 target, K와 V는 source embedding\r\n",
        "q = w_q(trg_emb)  # (B, T_L, d_model)\r\n",
        "k = w_k(src_emb)  # (B, S_L, d_model)\r\n",
        "v = w_v(src_emb)  # (B, S_L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 4])\n",
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeqjkVqkIdxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f6aee1-f4ec-4f05-8f83-fd0c66e6dfbf"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L) \r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\r\n",
        "\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQv4IINbItS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e1cbe9-924c-47c9-f4ef-0ca6619a81d2"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLCHeCbtJDy9"
      },
      "source": [
        "Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다."
      ]
    }
  ]
}