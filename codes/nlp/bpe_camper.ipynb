{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpe_camper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koQ-w1sV34sz"
      },
      "source": [
        "# Natural Language Processing\r\n",
        "## Assignment 4: Byte Pair Encoding\r\n",
        "\r\n",
        "### 1. Introduction\r\n",
        "\r\n",
        "- 일반적으로 하나의 단어에 대해 하나의 embedding을 생성할 경우 out-of-vocabulary(OOV)라는 치명적인 문제를 갖게 됩니다. 학습 데이터에서 등장하지 않은 단어가 나오는 경우 Unknown token으로 처리해주어 모델의 입력으로 넣게 되면서 전체적으로 모델의 성능이 저하될 수 있습니다. 반면 모든 단어의 embedding을 만들기에는 필요한 embedding parameter의 수가 지나치게 많습니다.\r\n",
        "이러한 문제를 해결하기 위해 컴퓨터가 이해하는 단어를 표현하는 데에 데이터 압축 알고리즘 중 하나인 byte pair encoding 기법을 적용한 sub-word tokenizaiton이라는 개념이 나타났습니다. \r\n",
        "- 본 과제에서는 byte pair encoding을 이용한 간단한 sub-word tokenizer를 구현해봅니다.\r\n",
        "과제 노트북의 지시사항과 각 함수의 docstring과 [논문](https://arxiv.org/pdf/1508.07909.pdf)의 3페이지 algorithm 1 참고하여 build_bpe 함수를 완성하고 모든 test case를 통과해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHKfcWxBgOlp",
        "outputId": "7c878388-e4a5-447c-e2b7-03ef69a5baa8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGuOpGM5VZD"
      },
      "source": [
        "## 2-1.build_bpe 함수를 완성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9go8KbPe3s-L"
      },
      "source": [
        "from typing import List, Dict, Set\r\n",
        "from itertools import chain\r\n",
        "import re\r\n",
        "from collections import defaultdict, Counter\r\n",
        "\r\n",
        "def get_stats(vocab):\r\n",
        "    pairs = defaultdict(int)\r\n",
        "    for word, freq in vocab.items():\r\n",
        "        symbols = word.split()\r\n",
        "        for i in range(len(symbols)-1):\r\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\r\n",
        "    return pairs\r\n",
        "\r\n",
        "def merge_vocab(pair, v_in):\r\n",
        "    v_out = {}\r\n",
        "    bigram = re.escape(' '.join(pair))\r\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\r\n",
        "    for word in v_in:\r\n",
        "        w_out = p.sub(''.join(pair), word)\r\n",
        "        v_out[w_out] = v_in[word]\r\n",
        "    return v_out\r\n",
        "\r\n",
        "\r\n",
        "def build_bpe(\r\n",
        "        corpus: List[str],\r\n",
        "        max_vocab_size: int\r\n",
        ") -> List[int]:\r\n",
        "    \"\"\" BPE Vocabulary Builder\r\n",
        "    Implement vocabulary builder for byte pair encoding.\r\n",
        "    Please sort your idx2word by subword length in descending manner.\r\n",
        "\r\n",
        "    Hint: Counter in collection library would be helpful\r\n",
        "\r\n",
        "    Note: If you convert sentences list to word frequence dictionary,\r\n",
        "          building speed is enhanced significantly because duplicated words are\r\n",
        "          preprocessed together\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    corpus -- List of words to build vocab\r\n",
        "    max_vocab_size -- The maximum size of vocab\r\n",
        "\r\n",
        "    Return:\r\n",
        "    idx2word -- Subword list\r\n",
        "    \"\"\"\r\n",
        "    # Special tokens\r\n",
        "    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\r\n",
        "    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\r\n",
        "    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\r\n",
        "    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\r\n",
        "    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\r\n",
        "    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\r\n",
        "\r\n",
        "    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word\r\n",
        "\r\n",
        "    # YOUR CODE HERE\r\n",
        "    idx2word = SPECIAL[:]\r\n",
        "    temp = []\r\n",
        "\r\n",
        "    dict_full = Counter(corpus)\r\n",
        "    dict_keys= dict_full.keys()\r\n",
        "    vocab = {}\r\n",
        "    for d in dict_full:\r\n",
        "        split_corpus = list(d)\r\n",
        "        split_corpus.append(WORD_END)\r\n",
        "        vocab[\" \".join(split_corpus)] = dict_full[d]\r\n",
        "\r\n",
        "    one_word = set()\r\n",
        "    for k in dict_keys:\r\n",
        "        one_word.update(list(k))\r\n",
        "\r\n",
        "    while len(temp) < (max_vocab_size - 6 - len(one_word)):\r\n",
        "        pairs = get_stats(vocab)\r\n",
        "        if len(pairs) == 0:\r\n",
        "            break\r\n",
        "        best = max(pairs, key=pairs.get)\r\n",
        "        vocab = merge_vocab(best, vocab)\r\n",
        "        temp.append(\"\".join(best))\r\n",
        "\r\n",
        "   \r\n",
        "    temp.extend(list(one_word))\r\n",
        "    temp.sort(key=len, reverse=True)\r\n",
        "\r\n",
        "    idx2word.extend(temp)\r\n",
        "    idx2word.append(WORD_END)\r\n",
        "\r\n",
        "    return idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0RBFPZbD8a",
        "outputId": "6c6362a3-1a05-4203-a80c-dc8ceb140ba7"
      },
      "source": [
        "corpus = ['abcde']\r\n",
        "vocab = build_bpe(corpus, max_vocab_size=15)\r\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '<unk>',\n",
              " '<cls>',\n",
              " '<sep>',\n",
              " '<msk>',\n",
              " 'abcde',\n",
              " 'abcd',\n",
              " 'abc',\n",
              " 'ab',\n",
              " 'e',\n",
              " 'd',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " '_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkNVTvrYSYlw",
        "outputId": "54fd0a4e-d5a1-4570-acd5-9ad09e38881e"
      },
      "source": [
        "test = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\r\n",
        "vocab = build_bpe(test, 24)\r\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '<unk>',\n",
              " '<cls>',\n",
              " '<sep>',\n",
              " '<msk>',\n",
              " 'newest_',\n",
              " 'est_',\n",
              " 'est',\n",
              " 'low',\n",
              " 'new',\n",
              " 'es',\n",
              " 'lo',\n",
              " 'ne',\n",
              " 'w',\n",
              " 'e',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'd',\n",
              " 'n',\n",
              " 'l',\n",
              " 'o',\n",
              " 'i',\n",
              " '_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJnrNlY5cjW"
      },
      "source": [
        "## 2-2. build_bpe 함수 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG6-h8Wv5KWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd0f575-21f1-428e-a833-968f24df79b1"
      },
      "source": [
        "#############################################\r\n",
        "# Helper functions below. DO NOT MODIFY!    #\r\n",
        "#############################################\r\n",
        "\r\n",
        "class BytePairEncoding(object):\r\n",
        "    \"\"\" Byte Pair Encoding class\r\n",
        "    We aren't gonna use this class for encoding. Because it is too slow......\r\n",
        "    We will use sentence piece Google have made.\r\n",
        "    Thus, this class is just for special token index reference.\r\n",
        "    \"\"\"\r\n",
        "    PAD_token = '<pad>'\r\n",
        "    PAD_token_idx = 0\r\n",
        "    UNK_token = '<unk>'\r\n",
        "    UNK_token_idx = 1\r\n",
        "    CLS_token = '<cls>'\r\n",
        "    CLS_token_idx = 2\r\n",
        "    SEP_token = '<sep>'\r\n",
        "    SEP_token_idx = 3\r\n",
        "    MSK_token = '<msk>'\r\n",
        "    MSK_token_idx = 4\r\n",
        "\r\n",
        "    WORD_END = '_'\r\n",
        "\r\n",
        "    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\r\n",
        "        self.idx2word = build_bpe(corpus, max_vocab_size)\r\n",
        "\r\n",
        "    def encode(self, sentence: List[str]) -> List[int]:\r\n",
        "        return encode(sentence, self.idx2word)\r\n",
        "\r\n",
        "    def decoder(self, tokens: List[int]) -> List[str]:\r\n",
        "        return decode(tokens, self.idx2word)\r\n",
        "\r\n",
        "\r\n",
        "#############################################\r\n",
        "# Testing functions below.                  #\r\n",
        "#############################################\r\n",
        "\r\n",
        "\r\n",
        "def test_build_bpe():\r\n",
        "    print(\"======Building BPE Vocab Test Case======\")\r\n",
        "    PAD = BytePairEncoding.PAD_token\r\n",
        "    UNK = BytePairEncoding.UNK_token\r\n",
        "    CLS = BytePairEncoding.CLS_token\r\n",
        "    SEP = BytePairEncoding.SEP_token\r\n",
        "    MSK = BytePairEncoding.MSK_token\r\n",
        "    WORD_END = BytePairEncoding.WORD_END\r\n",
        "\r\n",
        "    # First test\r\n",
        "    corpus = ['abcde']\r\n",
        "    vocab = build_bpe(corpus, max_vocab_size=15)\r\n",
        "    assert vocab[:5] == [PAD, UNK, CLS, SEP, MSK], \\\r\n",
        "        \"Please insert the special tokens properly\"\r\n",
        "    print(\"The first test passed!\")\r\n",
        "\r\n",
        "    # Second test\r\n",
        "    assert sorted(vocab[5:], key=len, reverse=True) == vocab[5:], \\\r\n",
        "        \"Please sort your idx2word by subword length in decsending manner.\"\r\n",
        "    print(\"The second test passed!\")\r\n",
        "\r\n",
        "    # Third test\r\n",
        "    corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\r\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=24))\r\n",
        "    assert vocab > {PAD, UNK, CLS, SEP, MSK, 'est_', 'low', 'newest_', \\\r\n",
        "                    'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w',\r\n",
        "                    WORD_END} and \\\r\n",
        "           \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The third test passed!\")\r\n",
        "\r\n",
        "    # forth test\r\n",
        "    corpus = ['aaaaaaaaaaaa', 'abababab']\r\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=13))\r\n",
        "    assert vocab == {PAD, UNK, CLS, SEP, MSK, 'aaaaaaaa', 'aaaa', 'abab', 'aa',\r\n",
        "                     'ab', 'a', 'b', WORD_END}, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The forth test passed!\")\r\n",
        "\r\n",
        "    # fifth test\r\n",
        "    corpus = ['abc', 'bcd']\r\n",
        "    vocab = build_bpe(corpus, max_vocab_size=10000)\r\n",
        "    assert len(vocab) == 15, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The fifth test passed!\")\r\n",
        "\r\n",
        "    print(\"All 5 tests passed!\")\r\n",
        "test_build_bpe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======Building BPE Vocab Test Case======\n",
            "The first test passed!\n",
            "The second test passed!\n",
            "The third test passed!\n",
            "The forth test passed!\n",
            "The fifth test passed!\n",
            "All 5 tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhKMIJ_gX-mT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}